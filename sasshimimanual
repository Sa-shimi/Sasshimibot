import random
import networkx as nx
import pandas as pd
from itertools import combinations
from random import randrange
import numpy as np
import matplotlib.pyplot as plt
import time




def checknumber(num):
    try:
        float(num)
        return True
    except ValueError:
        return False

def contaggion(num, par, steps, iters, primini):
        num = num
        par= par
        steps = steps
        iters = iters
        primini = primini
        m = 0

        def tutto():
            p = par

            F = nx.random_tree(n=num, create_using=nx.DiGraph, )
            G = nx.barabasi_albert_graph(num, p)

            def combine(arr, s):
                return list(combinations(arr, s))

            def Control_Centrality(F):
                length = len(F.nodes)
                i = 1

                def CC(F):
                    pippa = F.out_degree()
                    pippa1 = np.array(pippa)
                    pippa2 = pippa1[pippa1[:, 1] != 0]
                    F = F.subgraph(pippa2[:, 0])
                    return F

                def layer(F):
                    culo = [x for x in F.nodes if x not in CC(F).nodes]
                    banana = [i] * len(culo)
                    results = np.column_stack((culo, banana))
                    return results

                results = layer(F)
                while len(results) < length:
                    results = np.concatenate([results, layer(F)])
                    results = np.unique(results, axis=0)
                    i += 1
                    F = CC(F)
                    layer(F)
                return dict(zip(results[:, 0], (results[:, 1])))

            BC = nx.betweenness_centrality(F)

            isinstance(BC, dict)
            list(G.nodes)

            lll = combinations(G.nodes(), 2)
            lll = np.matrix(list(lll))
            cliques = list(nx.enumerate_all_cliques(G))
            triads = [i for i in cliques if len(i) == 3]
            simm = []
            for n in triads:
                for l in combine(n, 2):
                    simm.append(l)

            simmelian = []
            for i in simm:
                if i not in simmelian:
                    simmelian.append(i)

            simmelian = np.array(simmelian)

            A = simmelian[:, 0]
            B = simmelian[:, 1]
            A1 = lll[:, 0]
            B1 = lll[:, 1]
            df = pd.DataFrame(simmelian)
            numsimm = len(simmelian)

            BCA = []
            for x in A:
                for n in BC:
                    if x == n:
                        BCA.append(BC[n])
            BCB = []
            for y in B:
                for n in BC:
                    if y == n:
                        BCB.append(BC[n])

            df['BCA'] = BCA
            df['BCB'] = BCB
            df['AVG'] = (df['BCA'] + df['BCB']) / 2
            df['VAR'] = (((df['BCA'] - df['AVG']) ** 2 + (df['BCB'] - df['AVG']) ** 2) / 2)
            VAR = np.sum(df['VAR'])
            V = VAR / numsimm
            df1 = df[3:4]
            dff = []
            for k in G.nodes:
                for n in BC:
                    if k == n:
                        dff.append(BC[n])
            T = max(dff)

            H = pd.DataFrame(dff)
            H['BCMax'] = T
            H.rename(columns={0: 'BCentrality'}, inplace=True)
            H['Hier'] = (H['BCMax'] - H['BCentrality'])
            Q = np.mean(H['Hier'])
            v = Q * V

            BC1 = nx.betweenness_centrality(F)
            isinstance(BC1, dict)
            df1 = pd.DataFrame(lll)
            BC1A = []
            for x in A1:
                for n in BC1:
                    if x == n:
                        BC1A.append(BC1[n])
            BC1B = []
            for y in B1:
                for n in BC1:
                    if y == n:
                        BC1B.append(BC1[n])
            df1['BC1A'] = BC1A
            df1['BC1B'] = BC1B
            df1['AVG1'] = (df1['BC1A'] + df1['BC1B']) / 2
            df1['VAR1'] = ((df1['BC1A'] - df1['AVG1']) ** 2 + (df1['BC1B'] - df1['AVG1']) ** 2) / 2
            V1 = np.mean(df1['VAR1'])
            dff1 = []
            for k in G.nodes:
                for n in BC1:
                    if k == n:
                        dff1.append(BC1[n])
            T1 = max(dff1)
            H1 = pd.DataFrame(dff1)
            H1['BC1Max'] = T1
            H1.rename(columns={0: 'BCentrality1'}, inplace=True)
            H1['Hier1'] = (H1['BC1Max'] - H1['BCentrality1'])
            Vl = V / V1
            nx.set_edge_attributes(G, 0, name='imitation')
            for n in G.nodes:
                G.nodes[n]['imitation'] = randrange(1, 10) / 100

            sup = []
            for (i, j) in F.edges:
                sup.append((j, i))

            sub = F.edges

            nx.set_edge_attributes(G, 0, name='theta')
            for n in G.nodes:
                G.nodes[n]['theta'] = randrange(1, 10) / 500
            nx.set_edge_attributes(G, 0, name='ego')
            for n in G.nodes:
                G.nodes[n]['ego'] = randrange(4, 9) / 10
            nx.set_node_attributes(G, 0, name='relevance')
            primi = random.sample(range(0, num), primini)
            for n in primi:
                G.nodes[n]['relevance'] = 1

            nx.set_edge_attributes(G, 0, name='threshold')
            for n in G.nodes:
                G.nodes[n]['threshold'] = np.random.beta(1, 10)

            influence_matrix = np.matrix(np.zeros((num, num)))
            for i in G.nodes:
                for j in G.nodes:
                    if i != j:
                        if j in G.neighbors(i):
                            if BC[i] - BC[j] < 0:
                                influence_matrix[i, j] = (np.random.beta(1, 8) / len(list(G.neighbors(i))))
                            elif BC[i] - BC[j] > 0:
                                influence_matrix[i, j] = np.random.beta(3, 7) / len(list(G.neighbors(i)))
                            elif BC[i] - BC[j] == 0:
                                influence_matrix[i, j] = np.random.beta(4, 3) / len(list(G.neighbors(i)))
                influence_matrix[i, i] = (1 - influence_matrix[[i]].sum(axis=1))

            probability_matrix = np.matrix(np.zeros((num, num)))
            for i in G.nodes:
                for j in G.nodes:
                    if i == j:
                        probability_matrix[i, j] = 1
                    elif i != j:
                        if [i, j] in simmelian:
                            if BC[i] - BC[j] > 0:
                                probability_matrix[i, j] = np.random.beta(8, 1)
                            elif BC[i] - BC[j] < 0:
                                probability_matrix[i, j] = np.random.beta(8, 1)
                            elif BC[i] - BC[j] == 0:
                                probability_matrix[i, j] = np.random.beta(8, 1)
                        elif [i, j] not in simmelian:
                            if BC[i] - BC[j] > 0:
                                probability_matrix[i, j] = np.random.beta(4, 2)
                            elif BC[i] - BC[j] < 0:
                                probability_matrix[i, j] = np.random.beta(4, 2)
                            elif BC[i] - BC[j] == 0:
                                probability_matrix[i, j] = np.random.beta(4, 3)

            def degroot(G):
                K = G.copy()
                t = 1
                bellini = []

                def event():
                    P = K.copy()
                    for i in K.nodes:
                        opchange = [P.nodes[i]['relevance'] * influence_matrix[i, i]]
                        for j in K.neighbors(i):
                            if P.nodes[j]['relevance'] >= P.nodes[j]['threshold']:
                                if random.random() < probability_matrix[i, j]:
                                    opchange.append(influence_matrix[i, j])
                        opchange.append(P.nodes[i]['relevance'])
                        K.nodes[i]["relevance"] = np.sum(opchange) - K.nodes[i]['theta']
                    return K

                dati = [Vl]
                datelli = []
                while t != steps:
                    t += 1
                    event()
                    P = []
                    for n in K.nodes:
                        if K.nodes[n]['relevance'] >= K.nodes[n]['threshold']:
                            P.append(K.nodes[n]['relevance'])
                            if n not in bellini:
                                bellini.append(n)
                    dati.append((len(P) / num))
                    datelli.append((len(bellini) / num))

                return [dati, datelli]

            def contagion(G):
                K = G.copy()
                t = 1
                bellini = []

                def event():
                    P = K.copy()
                    for i in K.nodes:
                        opchange = [P.nodes[i]['relevance'] * influence_matrix[i, i]]
                        for j in K.neighbors(i):
                            if P.nodes[j]['relevance'] >= P.nodes[j]['threshold']:
                                if random.random() < probability_matrix[i, j]:
                                    opchange.append(influence_matrix[i, j] * P.nodes[j]['relevance'])
                                else:
                                    opchange.append(influence_matrix[i, j] * P.nodes[i]['relevance'])
                            if P.nodes[j]['relevance'] < P.nodes[j]['threshold']:
                                opchange.append(influence_matrix[i, j] * P.nodes[i]['relevance'])
                        K.nodes[i]["relevance"] = np.sum(opchange)

                    return K

                dati = [Vl]
                datelli = []
                while t != steps:
                    t += 1
                    event()
                    P = []
                    for n in K.nodes:
                        if K.nodes[n]['relevance'] >= K.nodes[n]['threshold']:
                            P.append(K.nodes[n]['relevance'])
                            if n not in bellini:
                                bellini.append(n)
                    dati.append((len(P) / num))
                    datelli.append((len(bellini) / num))

                return [dati, datelli]

            def bass(G):
                K = G.copy()
                t = 1
                bellini = []

                def event(G):
                    nonsimmy = []
                    simmy = []
                    for i in K.nodes:
                        if K.nodes[i] in simmelian:
                            if K.nodes[i]['relevance'] > K.nodes[i]['threshold']:
                                simmy.append(n)
                        if K.nodes[i] not in simmelian:
                            if K.nodes[i]['relevance'] > K.nodes[i]['threshold']:
                                nonsimmy.append(i)
                    prob = ((len(simmy) * Vl) + len(nonsimmy)) / len(G.nodes)
                    for i in K.nodes:
                        if K.nodes[i]['relevance'] <= K.nodes[i]['threshold']:
                            if random.random() < G.nodes[i]['imitation'] * prob:
                                K.nodes[i]['relevance'] = 1
                    return K

                dati = [Vl]
                datelli = []
                while t != steps:
                    t += 1
                    event(K)
                    P = []
                    for n in K.nodes:
                        if K.nodes[n]['relevance'] >= K.nodes[n]['threshold']:
                            P.append(K.nodes[n]['relevance'])
                            if n not in bellini:
                                bellini.append(n)
                    dati.append((len(P) / num))
                    datelli.append((len(bellini) / num))

                return [dati, datelli]

            return [degroot(G), contagion(G), bass(G), Vl]

        actualdegroot = []
        actualcontagion = []
        actualbass = []
        actualphi = []
        cumulativedegroot = []
        cumulativecontagion = []
        cumulativebass = []
        startTime = time.time()

        while m != iters:
            banana = tutto()
            executiontime = (time.time() - startTime)
            ETA = int((((executiontime / (m + 1)) * (iters - m))) / 60)
            print(int((m / iters) * 100), '% Estimated time remaining:',
                  ETA,
                  'minutes')
            m += 1
            actualdegroot.append(banana[0][0])
            actualcontagion.append(banana[1][0])
            actualbass.append(banana[2][0])
            actualphi.append(banana[3])
            cumulativedegroot.append((banana[0][1]))
            cumulativecontagion.append((banana[1][1]))
            cumulativebass.append((banana[2][1]))
        dat = np.array(actualdegroot)
        for n in dat:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('DeGroot model (as modified by Assenova 2018)')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()
        datcumulatividegroot = np.array(cumulativedegroot)
        for n in datcumulatividegroot:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1, label='Non-heterarchical')
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1, label='Heterarchical')
        plt.title('DeGroot model (as modified by Assenova 2018)')
        plt.xlabel('Time steps')
        plt.ylabel('Cumulative adoption rate')
        plt.grid()
        plt.show()
        lst = []
        n = len(tutto()[0][0])
        for i in range(n + 0):
            lst.append(i)
        minimo = []
        massimo = []
        media = []
        mediana = []

        for l in np.array(lst):
            minimo.append(dat[:, l].min())
            massimo.append(dat[:, l].max())
            media.append(dat[:, l].mean())
            mediana.append(np.median(dat[:, l], axis=0))

        plt.plot(minimo[1:], label='minimum')
        plt.plot(massimo[1:], label='maximum')
        plt.plot(media[1:], label='mean')
        plt.plot(mediana[1:], 'r--', label='median')
        plt.legend()
        plt.title('DeGroot model (as modified by Assenova 2018)')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()

        datcontagion = np.array(actualcontagion)
        for n in datcontagion:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1, label='Non-heterarchical')
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1, label='Heterarchical')
        plt.title('Piccione-Tolotti 2022')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()
        datcumulativicontagion = np.array(cumulativecontagion)
        for n in datcumulativicontagion:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1, label='Non-heterarchical')
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1, label='Heterarchical')
        plt.title('Piccione-Tolotti 2022')
        plt.xlabel('Time steps')
        plt.ylabel('Cumulative adoption rate')
        plt.grid()
        plt.show()
        lstcontagion = []
        n = len(tutto()[1][0])
        for i in range(n + 0):
            lstcontagion.append(i)
        minimo = []
        massimo = []
        media = []
        mediana = []
        for l in np.array(lstcontagion):
            minimo.append(datcontagion[:, l].min())
            massimo.append(datcontagion[:, l].max())
            media.append(datcontagion[:, l].mean())
            mediana.append(np.median(datcontagion[:, l], axis=0))

        plt.plot(minimo[1:], label='minimum')
        plt.plot(massimo[1:], label='maximum')
        plt.plot(media[1:], label='mean')
        plt.plot(mediana[1:], 'r--', label='median')
        plt.legend()
        plt.title('Piccione-Tolotti 2022')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()

        datbass = np.array(actualbass)
        for n in datbass:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1, label='Non-heterarchical')
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1, label='Heterarchical')
        plt.title('Bass (1969) model')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()
        datcumulativibass = np.array(cumulativebass)
        for n in datcumulativibass:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1, label='Non-heterarchical')
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1, label='Heterarchical')
        plt.title('Bass (1969) model')
        plt.xlabel('Time steps')
        plt.ylabel('Cumulative adoption rate')
        plt.grid()
        plt.show()
        lstbass = []
        n = len(tutto()[2][0])
        for i in range(n + 0):
            lstbass.append(i)
        minimo = []
        massimo = []
        media = []
        mediana = []

        for l in np.array(lstbass):
            minimo.append(datbass[:, l].min())
            massimo.append(datbass[:, l].max())
            media.append(datbass[:, l].mean())
            mediana.append(np.median(datbass[:, l], axis=0))

        plt.plot(minimo[1:], label='minimum')
        plt.plot(massimo[1:], label='maximum')
        plt.plot(media[1:], label='mean')
        plt.plot(mediana[1:], 'r--', label='median')
        plt.legend()
        plt.title('Bass (1969) model')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()

def parameter_analys_fix(num, par, steps, iters, thr, primini):
        p = par
        thr = thr
        m = 0
        primi = random.sample(range(0, num), primini)
        F = nx.random_tree(n=num, create_using=nx.DiGraph, )
        G = nx.barabasi_albert_graph(num, p)

        def tutto():

            nx.set_node_attributes(G, 0, name='relevance')
            for n in primi:
                G.nodes[n]['relevance'] = 1

            def combine(arr, s):
                return list(combinations(arr, s))

            def Control_Centrality(F):
                length = len(F.nodes)
                i = 1

                def CC(F):
                    pippa = F.out_degree()
                    pippa1 = np.array(pippa)
                    pippa2 = pippa1[pippa1[:, 1] != 0]
                    F = F.subgraph(pippa2[:, 0])
                    return F

                def layer(F):
                    culo = [x for x in F.nodes if x not in CC(F).nodes]
                    banana = [i] * len(culo)
                    results = np.column_stack((culo, banana))
                    return results

                results = layer(F)
                while len(results) < length:
                    results = np.concatenate([results, layer(F)])
                    results = np.unique(results, axis=0)
                    i += 1
                    F = CC(F)
                    layer(F)
                return dict(zip(results[:, 0], (results[:, 1])))

            BC = nx.betweenness_centrality(F)

            isinstance(BC, dict)
            list(G.nodes)

            lll = combinations(G.nodes(), 2)
            lll = np.matrix(list(lll))
            cliques = list(nx.enumerate_all_cliques(G))
            triads = [i for i in cliques if len(i) == 3]
            simm = []
            for n in triads:
                for l in combine(n, 2):
                    simm.append(l)

            simmelian = []
            for i in simm:
                if i not in simmelian:
                    simmelian.append(i)

            simmelian = np.array(simmelian)

            A = simmelian[:, 0]
            B = simmelian[:, 1]
            A1 = lll[:, 0]
            B1 = lll[:, 1]
            df = pd.DataFrame(simmelian)
            numsimm = len(simmelian)

            BCA = []
            for x in A:
                for n in BC:
                    if x == n:
                        BCA.append(BC[n])
            BCB = []
            for y in B:
                for n in BC:
                    if y == n:
                        BCB.append(BC[n])

            df['BCA'] = BCA
            df['BCB'] = BCB
            df['AVG'] = (df['BCA'] + df['BCB']) / 2
            df['VAR'] = (((df['BCA'] - df['AVG']) ** 2 + (df['BCB'] - df['AVG']) ** 2) / 2)
            VAR = np.sum(df['VAR'])
            V = VAR / numsimm
            df1 = df[3:4]
            dff = []
            for k in G.nodes:
                for n in BC:
                    if k == n:
                        dff.append(BC[n])
            T = max(dff)

            H = pd.DataFrame(dff)
            H['BCMax'] = T
            H.rename(columns={0: 'BCentrality'}, inplace=True)
            H['Hier'] = (H['BCMax'] - H['BCentrality'])
            Q = np.mean(H['Hier'])
            v = Q * V

            BC1 = nx.betweenness_centrality(F)
            isinstance(BC1, dict)
            df1 = pd.DataFrame(lll)
            BC1A = []
            for x in A1:
                for n in BC1:
                    if x == n:
                        BC1A.append(BC1[n])
            BC1B = []
            for y in B1:
                for n in BC1:
                    if y == n:
                        BC1B.append(BC1[n])
            df1['BC1A'] = BC1A
            df1['BC1B'] = BC1B
            df1['AVG1'] = (df1['BC1A'] + df1['BC1B']) / 2
            df1['VAR1'] = ((df1['BC1A'] - df1['AVG1']) ** 2 + (df1['BC1B'] - df1['AVG1']) ** 2) / 2
            V1 = np.mean(df1['VAR1'])
            dff1 = []
            for k in G.nodes:
                for n in BC1:
                    if k == n:
                        dff1.append(BC1[n])
            T1 = max(dff1)
            H1 = pd.DataFrame(dff1)
            H1['BC1Max'] = T1
            H1.rename(columns={0: 'BCentrality1'}, inplace=True)
            H1['Hier1'] = (H1['BC1Max'] - H1['BCentrality1'])
            Vl = V / V1
            nx.set_edge_attributes(G, 0, name='imitation')
            for n in G.nodes:
                G.nodes[n]['imitation'] = randrange(1, 10) / 100

            sup = []
            for (i, j) in F.edges:
                sup.append((j, i))

            sub = F.edges

            nx.set_edge_attributes(G, 0, name='theta')
            for n in G.nodes:
                G.nodes[n]['theta'] = randrange(1, 10) / 500
            nx.set_edge_attributes(G, 0, name='ego')
            for n in G.nodes:
                G.nodes[n]['ego'] = randrange(4, 9) / 10

            def modelling(meansup, meansequal, meansdown, meannup, meannequal, meanndown, sigmas, sigman,
                          ):
                """
                sup is when the information is going upward, down when it goes downward. if the information is going
                upword, it means that subject i is the superior


                for mean 0.8 is max and 0.2 minimum
                for sigma 0.05 is max and 0.01 is minimum
                """
                influence_matrix = np.matrix(np.zeros((num, num)))
                for i in G.nodes:
                    for j in G.nodes:
                        if i != j:
                            if j in G.neighbors(i):
                                if BC[i] - BC[j] < 0:
                                    influence_matrix[i, j] = np.random.normal(meanndown, sigman) / len(
                                        list(G.neighbors(i)))
                                elif BC[i] - BC[j] > 0:
                                    influence_matrix[i, j] = np.random.normal(meannup, sigman) / len(
                                        list(G.neighbors(i)))
                                elif BC[i] - BC[j] == 0:
                                    influence_matrix[i, j] = np.random.normal(meannequal, sigman) / len(
                                        list(G.neighbors(i)))
                        if influence_matrix[i, j] < 0:
                            influence_matrix[i, j] = 0
                        elif influence_matrix[i, j] > 1:
                            influence_matrix[i, j] = 1
                    influence_matrix[i, i] = (1 - influence_matrix[[i]].sum(axis=1))

                probability_matrix = np.matrix(np.zeros((num, num)))
                for i in G.nodes:
                    for j in G.nodes:
                        if i == j:
                            probability_matrix[i, j] = 1
                        elif i != j:
                            if [i, j] in simmelian:
                                if BC[i] - BC[j] > 0:
                                    probability_matrix[i, j] = np.random.normal(meansup, sigmas)
                                elif BC[i] - BC[j] < 0:
                                    probability_matrix[i, j] = np.random.normal(meansdown, sigmas)
                                elif BC[i] - BC[j] == 0:
                                    probability_matrix[i, j] = np.random.normal(meansequal, sigmas)
                            elif [i, j] not in simmelian:
                                if BC[i] - BC[j] > 0:
                                    probability_matrix[i, j] = np.random.normal(meannup, sigman)
                                elif BC[i] - BC[j] < 0:
                                    probability_matrix[i, j] = np.random.normal(meanndown, sigman)
                                elif BC[i] - BC[j] == 0:
                                    probability_matrix[i, j] = np.random.normal(meannequal, sigman)
                        if probability_matrix[i, j] < 0:
                            probability_matrix[i, j] = 0
                        elif probability_matrix[i, j] > 1:
                            probability_matrix[i, j] = 1

                nx.set_edge_attributes(G, 0, name='threshold')
                for n in G.nodes:
                    G.nodes[n]['threshold'] = thr


                K = G.copy()
                t = 0
                bellini = []

                def event():
                    P = K.copy()
                    for i in K.nodes:
                        opchange = [P.nodes[i]['relevance'] * influence_matrix[i, i]]
                        for j in K.neighbors(i):
                            if P.nodes[j]['relevance'] >= P.nodes[j]['threshold']:
                                if random.random() < probability_matrix[i, j]:
                                    opchange.append(influence_matrix[i, j] * P.nodes[j]['relevance'])
                                else:
                                    opchange.append(influence_matrix[i, j] * P.nodes[i]['relevance'])
                            if P.nodes[j]['relevance'] < P.nodes[j]['threshold']:
                                opchange.append(influence_matrix[i, j] * P.nodes[i]['relevance'])
                        K.nodes[i]["relevance"] = np.sum(opchange)
                        if K.nodes[i]["relevance"] > 1:
                            K.nodes[i]["relevance"] = 1

                    return K

                dati = [Vl, (primini / num)]
                datelli = [(primini / num)]
                datini = [0]
                while t != steps:

                    event()
                    P = []
                    P2 = []
                    for n in K.nodes:
                        if K.nodes[n]['relevance'] >= K.nodes[n]['threshold']:
                            P.append(K.nodes[n]['relevance'])
                            if n not in bellini:
                                bellini.append(n)
                        P2.append(K.nodes[n]['relevance'])
                    dati.append((len(P) / num))
                    datelli.append((len(bellini) / num))
                    datini.append(np.mean(P2))
                    t += 1

                return [dati, datelli, datini]

            """
                    sup is when the information is going upward, down when it goes downward. if the information is going
                    upword, it means that subject i is the superior


                    for mean 0.8 is max and 0.2 minimum
                    for sigma 0.05 is max and 0.01 is minimum
                    """

            return [modelling(meansup=1, meansequal=1, meansdown=1, meannup=1, meannequal=1, meanndown=1,
                              sigmas=0, sigman=0),
                    modelling(meansup=1, meansequal=1, meansdown=1, meannup=1, meannequal=1, meanndown=1,
                              sigmas=0, sigman=0),
                    modelling(meansup=0.0, meansequal=0.4, meansdown=0.6, meannup=0.2, meannequal=0.4, meanndown=0.6,

                              sigmas=0, sigman=0),
                    modelling(meansup=0.0, meansequal=0.4, meansdown=0.6, meannup=0.2, meannequal=0.4, meanndown=0.6,
                              sigmas=0.01, sigman=0.05),
                    modelling(meansup=0.2, meansequal=0.7, meansdown=0.8, meannup=0.2, meannequal=0.4, meanndown=0.6,
                              sigmas=0.01, sigman=0.05), Vl]

        """
        the first one is with no thresholod
        the second one is with average threshold
        the third one includes differences between levels
        the fourth one considers divergencies between simmelian and non simmelian
        """

        DATONIA = []
        DATONIB = []
        DATONIC = []
        DATONID = []
        DATONIE = []
        DATONIphi = []
        cumulativeA = []
        cumulativeB = []
        cumulativeC = []
        cumulativeD = []
        cumulativeE = []
        opinionA = []
        opinionB = []
        opinionC = []
        opinionD = []
        opinionE = []

        startTime = time.time()

        while m != iters:
            banana = tutto()
            executiontime = (time.time() - startTime)
            ETA = int((((executiontime / (m + 1)) * (iters - m))) / 60)
            print(int((m / iters) * 100), '% Estimated time remaining:',
                  ETA,
                  'minutes')
            m += 1
            DATONIA.append(banana[0][0])
            DATONIB.append(banana[1][0])
            DATONIC.append(banana[2][0])
            DATONID.append(banana[3][0])
            DATONIE.append(banana[4][0])
            DATONIphi.append(banana[5])
            cumulativeA.append((banana[0][1]))
            cumulativeB.append((banana[1][1]))
            cumulativeC.append((banana[2][1]))
            cumulativeD.append((banana[3][1]))
            cumulativeE.append((banana[4][1]))
            opinionA.append((banana[0][2]))
            opinionB.append((banana[1][2]))
            opinionC.append((banana[2][2]))
            opinionD.append((banana[3][2]))
            opinionE.append((banana[4][2]))

        dat = np.array(DATONIA)
        for n in dat:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 1: Every parameter is fixed')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datcumulativeA = np.array(cumulativeA)
        for n in datcumulativeA:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 1: Every parameter is fixed')
        plt.xlabel('Time steps')
        plt.ylabel('Cumulative adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datopinionA = np.array(opinionA)
        for n in datopinionA:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 1: Every parameter is fixed')
        plt.xlabel('Time steps')
        plt.ylabel('Average Opinion')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        lstDATONIA = []
        n = len(tutto()[1][0])
        for i in range(n + 0):
            lstDATONIA.append(i)
        minimo = []
        massimo = []
        media = []
        mediana = []
        for l in np.array(lstDATONIA):
            minimo.append(dat[:, l].min())
            massimo.append(dat[:, l].max())
            media.append(dat[:, l].mean())
            mediana.append(np.median(dat[:, l], axis=0))
        plt.plot(minimo[1:], label='minimum')
        plt.plot(massimo[1:], label='maximum')
        plt.plot(media[1:], label='mean')
        plt.plot(mediana[1:], 'r--', label='median')
        plt.legend()
        plt.title('step 1: Every parameter is fixed')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()

        dat = np.array(DATONIB)
        for n in dat:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 2: a (random) threshold is assigned')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datcumulativeB = np.array(cumulativeB)
        for n in datcumulativeB:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 2: a (random) threshold is assigned')
        plt.xlabel('Time steps')
        plt.ylabel('Cumulative adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datopinionB = np.array(opinionB)
        for n in datopinionB:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 2: a (random) threshold is assigned')
        plt.xlabel('Time steps')
        plt.ylabel('Average Opinion')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        lstDATONIB = []
        n = len(tutto()[1][0])
        for i in range(n + 0):
            lstDATONIB.append(i)
        minimo = []
        massimo = []
        media = []
        mediana = []
        for l in np.array(lstDATONIB):
            minimo.append(dat[:, l].min())
            massimo.append(dat[:, l].max())
            media.append(dat[:, l].mean())
            mediana.append(np.median(dat[:, l], axis=0))
        plt.plot(minimo[1:], label='minimum')
        plt.plot(massimo[1:], label='maximum')
        plt.plot(media[1:], label='mean')
        plt.plot(mediana[1:], 'r--', label='median')
        plt.legend()
        plt.title('step 2: a (random) threshold is assigned')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()

        dat = np.array(DATONIC)
        for n in dat:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 3: introducing differences between levels')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datcumulativeC = np.array(cumulativeC)
        for n in datcumulativeC:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 3: introducing differences between levels')
        plt.xlabel('Time steps')
        plt.ylabel('Cumulative adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datopinionC = np.array(opinionC)
        for n in datopinionC:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 3: introducing differences between levels')
        plt.xlabel('Time steps')
        plt.ylabel('Average Opinion')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        lstDATONIC = []
        n = len(tutto()[1][0])
        for i in range(n + 0):
            lstDATONIC.append(i)
        minimo = []
        massimo = []
        media = []
        mediana = []
        for l in np.array(lstDATONIC):
            minimo.append(dat[:, l].min())
            massimo.append(dat[:, l].max())
            media.append(dat[:, l].mean())
            mediana.append(np.median(dat[:, l], axis=0))
        plt.plot(minimo[1:], label='minimum')
        plt.plot(massimo[1:], label='maximum')
        plt.plot(media[1:], label='mean')
        plt.plot(mediana[1:], 'r--', label='median')
        plt.legend()
        plt.title('step 3: introducing differences between levels')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()

        dat = np.array(DATONID)
        for n in dat:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 4: introducing differences between Simmelian and non simmelian (sigma)')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datcumulativeD = np.array(cumulativeD)
        for n in datcumulativeD:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 4: introducing differences between Simmelian and non simmelian (sigma)')
        plt.xlabel('Time steps')
        plt.ylabel('Cumulative adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datopinionD = np.array(opinionD)
        for n in datopinionD:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 4: introducing differences between Simmelian and non simmelian (sigma)')
        plt.xlabel('Time steps')
        plt.ylabel('Average Opinion')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        lstDATONID = []
        n = len(tutto()[1][0])
        for i in range(n + 0):
            lstDATONID.append(i)
        minimo = []
        massimo = []
        media = []
        mediana = []
        for l in np.array(lstDATONID):
            minimo.append(dat[:, l].min())
            massimo.append(dat[:, l].max())
            media.append(dat[:, l].mean())
            mediana.append(np.median(dat[:, l], axis=0))
        plt.plot(minimo[1:], label='minimum')
        plt.plot(massimo[1:], label='maximum')
        plt.plot(media[1:], label='mean')
        plt.plot(mediana[1:], 'r--', label='median')
        plt.legend()
        plt.title('step 4: introducing differences between Simmelian and non simmelian (sigma)')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()

        dat = np.array(DATONIE)
        for n in dat:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 5: introducing differences between Simmelian and non simmelian (sigma and mean)')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datcumulativeE = np.array(cumulativeE)
        for n in datcumulativeE:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 5: introducing differences between Simmelian and non simmelian (sigma and mean)')
        plt.xlabel('Time steps')
        plt.ylabel('Cumulative adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datopinionE = np.array(opinionE)
        for n in datopinionE:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 5: introducing differences between Simmelian and non simmelian (sigma and mean')
        plt.xlabel('Time steps')
        plt.ylabel('Average Opinion')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.show()
        plt.grid()
        plt.cla()
        lstDATONIE = []
        n = len(tutto()[1][0])
        for i in range(n + 0):
            lstDATONIE.append(i)
        minimo = []
        massimo = []
        media = []
        mediana = []
        for l in np.array(lstDATONIE):
            minimo.append(dat[:, l].min())
            massimo.append(dat[:, l].max())
            media.append(dat[:, l].mean())
            mediana.append(np.median(dat[:, l], axis=0))
        plt.plot(minimo[1:], label='minimum')
        plt.plot(massimo[1:], label='maximum')
        plt.plot(media[1:], label='mean')
        plt.plot(mediana[1:], 'r--', label='median')
        plt.legend()
        plt.title('step 5: introducing differences between Simmelian and non simmelian (sigma and mean)')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()


def parameter_analysis(num, par, steps, iters, thr, primini):
        m = 0

        p = par

        def tutto():
            F = nx.random_tree(n=num, create_using=nx.DiGraph, )
            G = nx.barabasi_albert_graph(num, p)
            nx.set_node_attributes(G, 0, name='relevance')
            primi = random.sample(range(0, num), primini)
            for n in primi:
                G.nodes[n]['relevance'] = 1

            def combine(arr, s):
                return list(combinations(arr, s))

            def Control_Centrality(F):
                length = len(F.nodes)
                i = 1

                def CC(F):
                    pippa = F.out_degree()
                    pippa1 = np.array(pippa)
                    pippa2 = pippa1[pippa1[:, 1] != 0]
                    F = F.subgraph(pippa2[:, 0])
                    return F

                def layer(F):
                    culo = [x for x in F.nodes if x not in CC(F).nodes]
                    banana = [i] * len(culo)
                    results = np.column_stack((culo, banana))
                    return results

                results = layer(F)
                while len(results) < length:
                    results = np.concatenate([results, layer(F)])
                    results = np.unique(results, axis=0)
                    i += 1
                    F = CC(F)
                    layer(F)
                return dict(zip(results[:, 0], (results[:, 1])))

            BC = nx.betweenness_centrality(F)

            isinstance(BC, dict)
            list(G.nodes)

            lll = combinations(G.nodes(), 2)
            lll = np.matrix(list(lll))
            cliques = list(nx.enumerate_all_cliques(G))
            triads = [i for i in cliques if len(i) == 3]
            simm = []
            for n in triads:
                for l in combine(n, 2):
                    simm.append(l)

            simmelian = []
            for i in simm:
                if i not in simmelian:
                    simmelian.append(i)

            simmelian = np.array(simmelian)

            A = simmelian[:, 0]
            B = simmelian[:, 1]
            A1 = lll[:, 0]
            B1 = lll[:, 1]
            df = pd.DataFrame(simmelian)
            numsimm = len(simmelian)

            BCA = []
            for x in A:
                for n in BC:
                    if x == n:
                        BCA.append(BC[n])
            BCB = []
            for y in B:
                for n in BC:
                    if y == n:
                        BCB.append(BC[n])

            df['BCA'] = BCA
            df['BCB'] = BCB
            df['AVG'] = (df['BCA'] + df['BCB']) / 2
            df['VAR'] = (((df['BCA'] - df['AVG']) ** 2 + (df['BCB'] - df['AVG']) ** 2) / 2)
            VAR = np.sum(df['VAR'])
            V = VAR / numsimm
            df1 = df[3:4]
            dff = []
            for k in G.nodes:
                for n in BC:
                    if k == n:
                        dff.append(BC[n])
            T = max(dff)

            H = pd.DataFrame(dff)
            H['BCMax'] = T
            H.rename(columns={0: 'BCentrality'}, inplace=True)
            H['Hier'] = (H['BCMax'] - H['BCentrality'])
            Q = np.mean(H['Hier'])
            v = Q * V

            BC1 = nx.betweenness_centrality(F)
            isinstance(BC1, dict)
            df1 = pd.DataFrame(lll)
            BC1A = []
            for x in A1:
                for n in BC1:
                    if x == n:
                        BC1A.append(BC1[n])
            BC1B = []
            for y in B1:
                for n in BC1:
                    if y == n:
                        BC1B.append(BC1[n])
            df1['BC1A'] = BC1A
            df1['BC1B'] = BC1B
            df1['AVG1'] = (df1['BC1A'] + df1['BC1B']) / 2
            df1['VAR1'] = ((df1['BC1A'] - df1['AVG1']) ** 2 + (df1['BC1B'] - df1['AVG1']) ** 2) / 2
            V1 = np.mean(df1['VAR1'])
            dff1 = []
            for k in G.nodes:
                for n in BC1:
                    if k == n:
                        dff1.append(BC1[n])
            T1 = max(dff1)
            H1 = pd.DataFrame(dff1)
            H1['BC1Max'] = T1
            H1.rename(columns={0: 'BCentrality1'}, inplace=True)
            H1['Hier1'] = (H1['BC1Max'] - H1['BCentrality1'])
            Vl = V / V1
            nx.set_edge_attributes(G, 0, name='imitation')
            for n in G.nodes:
                G.nodes[n]['imitation'] = randrange(1, 10) / 100

            sup = []
            for (i, j) in F.edges:
                sup.append((j, i))

            sub = F.edges

            nx.set_edge_attributes(G, 0, name='theta')
            for n in G.nodes:
                G.nodes[n]['theta'] = randrange(1, 10) / 500
            nx.set_edge_attributes(G, 0, name='ego')
            for n in G.nodes:
                G.nodes[n]['ego'] = randrange(4, 9) / 10

            def modelling(meansup, meansequal, meansdown, meannup, meannequal, meanndown, sigmas, sigman,
                          ):
                """
                sup is when the information is going upward, down when it goes downward. if the information is going
                upword, it means that subject i is the superior


                for mean 0.8 is max and 0.2 minimum
                for sigma 0.05 is max and 0.01 is minimum
                """
                influence_matrix = np.matrix(np.zeros((num, num)))
                for i in G.nodes:
                    for j in G.nodes:
                        if i != j:
                            if j in G.neighbors(i):
                                if BC[i] - BC[j] < 0:
                                    influence_matrix[i, j] = np.random.normal(meanndown, sigman) / len(
                                        list(G.neighbors(i)))
                                elif BC[i] - BC[j] > 0:
                                    influence_matrix[i, j] = np.random.normal(meannup, sigman) / len(
                                        list(G.neighbors(i)))
                                elif BC[i] - BC[j] == 0:
                                    influence_matrix[i, j] = np.random.normal(meannequal, sigman) / len(
                                        list(G.neighbors(i)))
                        if influence_matrix[i, j] < 0:
                            influence_matrix[i, j] = 0
                        elif influence_matrix[i, j] > 1:
                            influence_matrix[i, j] = 1
                    influence_matrix[i, i] = (1 - influence_matrix[[i]].sum(axis=1))

                probability_matrix = np.matrix(np.zeros((num, num)))
                for i in G.nodes:
                    for j in G.nodes:
                        if i == j:
                            probability_matrix[i, j] = 1
                        elif i != j:
                            if [i, j] in simmelian:
                                if BC[i] - BC[j] > 0:
                                    probability_matrix[i, j] = np.random.normal(meansup, sigmas)
                                elif BC[i] - BC[j] < 0:
                                    probability_matrix[i, j] = np.random.normal(meansdown, sigmas)
                                elif BC[i] - BC[j] == 0:
                                    probability_matrix[i, j] = np.random.normal(meansequal, sigmas)
                            elif [i, j] not in simmelian:
                                if BC[i] - BC[j] > 0:
                                    probability_matrix[i, j] = np.random.normal(meannup, sigman)
                                elif BC[i] - BC[j] < 0:
                                    probability_matrix[i, j] = np.random.normal(meanndown, sigman)
                                elif BC[i] - BC[j] == 0:
                                    probability_matrix[i, j] = np.random.normal(meannequal, sigman)
                        if probability_matrix[i, j] < 0:
                            probability_matrix[i, j] = 0
                        elif probability_matrix[i, j] > 1:
                            probability_matrix[i, j] = 1

                nx.set_edge_attributes(G, 0, name='threshold')
                for n in G.nodes:
                    G.nodes[n]['threshold'] = thr

                K = G.copy()
                t = 0
                bellini = []

                def event():
                    P = K.copy()
                    for i in K.nodes:
                        opchange = [P.nodes[i]['relevance'] * influence_matrix[i, i]]
                        for j in K.neighbors(i):
                            if P.nodes[j]['relevance'] >= P.nodes[j]['threshold']:
                                if random.random() < probability_matrix[i, j]:
                                    opchange.append(influence_matrix[i, j] * P.nodes[j]['relevance'])
                                else:
                                    opchange.append(influence_matrix[i, j] * P.nodes[i]['relevance'])
                            if P.nodes[j]['relevance'] < P.nodes[j]['threshold']:
                                opchange.append(influence_matrix[i, j] * P.nodes[i]['relevance'])
                        K.nodes[i]["relevance"] = np.sum(opchange)
                        if K.nodes[i]["relevance"] > 1:
                            K.nodes[i]["relevance"] = 1

                    return K

                dati = [Vl, (primini / num)]
                datelli = [(primini / num)]
                datini = [0]
                while t != steps:

                    event()
                    P = []
                    P2 = []
                    for n in K.nodes:
                        if K.nodes[n]['relevance'] >= K.nodes[n]['threshold']:
                            P.append(K.nodes[n]['relevance'])
                            if n not in bellini:
                                bellini.append(n)
                        P2.append(K.nodes[n]['relevance'])
                    dati.append((len(P) / num))
                    datelli.append((len(bellini) / num))
                    datini.append(np.mean(P2))
                    t += 1

                return [dati, datelli, datini]
            

            """
                    sup is when the information is going upward, down when it goes downward. if the information is going
                    upword, it means that subject i is the superior


                    for mean 0.8 is max and 0.2 minimum
                    for sigma 0.05 is max and 0.01 is minimum
                    """

            return [modelling(meansup=1, meansequal=1, meansdown=1, meannup=1, meannequal=1, meanndown=1,
                              sigmas=0, sigman=0),
                    modelling(meansup=1, meansequal=1, meansdown=1, meannup=1, meannequal=1, meanndown=1,
                              sigmas=0, sigman=0),
                    modelling(meansup=0.0, meansequal=0.4, meansdown=0.6, meannup=0.2, meannequal=0.4, meanndown=0.6,

                              sigmas=0, sigman=0),
                    modelling(meansup=0.1, meansequal=0.2, meansdown=0.3, meannup=0.2, meannequal=0.4, meanndown=0.6,
                              sigmas=0.01, sigman=0.05),
                    modelling(meansup=0.1, meansequal=0.2, meansdown=0.3, meannup=0.2, meannequal=0.4, meanndown=0.6,
                              sigmas=0.01, sigman=0.05), Vl]

        """
        the first one is with no thresholod
        the second one is with average threshold
        the third one includes differences between levels
        the fourth one considers divergencies between simmelian and non simmelian
        """

        DATONIA = []
        DATONIB = []
        DATONIC = []
        DATONID = []
        DATONIE = []
        DATONIphi = []
        cumulativeA = []
        cumulativeB = []
        cumulativeC = []
        cumulativeD = []
        cumulativeE = []
        opinionA = []
        opinionB = []
        opinionC = []
        opinionD = []
        opinionE = []

        startTime = time.time()

        while m != iters:
            banana = tutto()
            executiontime = (time.time() - startTime)
            ETA = int((((executiontime / (m + 1)) * (iters - m))) / 60)
            print(int((m / iters) * 100), '% Estimated time remaining:',
                  ETA,
                  'minutes')
            m += 1
            DATONIA.append(banana[0][0])
            DATONIB.append(banana[1][0])
            DATONIC.append(banana[2][0])
            DATONID.append(banana[3][0])
            DATONIE.append(banana[4][0])
            DATONIphi.append(banana[5])
            cumulativeA.append((banana[0][1]))
            cumulativeB.append((banana[1][1]))
            cumulativeC.append((banana[2][1]))
            cumulativeD.append((banana[3][1]))
            cumulativeE.append((banana[4][1]))
            opinionA.append((banana[0][2]))
            opinionB.append((banana[1][2]))
            opinionC.append((banana[2][2]))
            opinionD.append((banana[3][2]))
            opinionE.append((banana[4][2]))

        dat = np.array(DATONIA)
        for n in dat:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 1: Every parameter is fixed')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datcumulativeA = np.array(cumulativeA)
        for n in datcumulativeA:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 1: Every parameter is fixed')
        plt.xlabel('Time steps')
        plt.ylabel('Cumulative adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datopinionA = np.array(opinionA)
        for n in datopinionA:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 1: Every parameter is fixed')
        plt.xlabel('Time steps')
        plt.ylabel('Average Opinion')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        lstDATONIA = []
        n = len(tutto()[1][0])
        for i in range(n + 0):
            lstDATONIA.append(i)
        minimo = []
        massimo = []
        media = []
        mediana = []
        for l in np.array(lstDATONIA):
            minimo.append(dat[:, l].min())
            massimo.append(dat[:, l].max())
            media.append(dat[:, l].mean())
            mediana.append(np.median(dat[:, l], axis=0))
        plt.plot(minimo[1:], label='minimum')
        plt.plot(massimo[1:], label='maximum')
        plt.plot(media[1:], label='mean')
        plt.plot(mediana[1:], 'r--', label='median')
        plt.legend()
        plt.title('step 1: Every parameter is fixed')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()

        dat = np.array(DATONIB)
        for n in dat:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 2: a (random) threshold is assigned')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datcumulativeB = np.array(cumulativeB)
        for n in datcumulativeB:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 2: a (random) threshold is assigned')
        plt.xlabel('Time steps')
        plt.ylabel('Cumulative adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datopinionB = np.array(opinionB)
        for n in datopinionB:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 2: a (random) threshold is assigned')
        plt.xlabel('Time steps')
        plt.ylabel('Average Opinion')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        lstDATONIB = []
        n = len(tutto()[1][0])
        for i in range(n + 0):
            lstDATONIB.append(i)
        minimo = []
        massimo = []
        media = []
        mediana = []
        for l in np.array(lstDATONIB):
            minimo.append(dat[:, l].min())
            massimo.append(dat[:, l].max())
            media.append(dat[:, l].mean())
            mediana.append(np.median(dat[:, l], axis=0))
        plt.plot(minimo[1:], label='minimum')
        plt.plot(massimo[1:], label='maximum')
        plt.plot(media[1:], label='mean')
        plt.plot(mediana[1:], 'r--', label='median')
        plt.legend()
        plt.title('step 2: a (random) threshold is assigned')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()
        dat = np.array(DATONIC)
        for n in dat:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 3: introducing differences between levels')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datcumulativeC = np.array(cumulativeC)
        for n in datcumulativeC:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 3: introducing differences between levels')
        plt.xlabel('Time steps')
        plt.ylabel('Cumulative adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datopinionC = np.array(opinionC)
        for n in datopinionC:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 3: introducing differences between levels')
        plt.xlabel('Time steps')
        plt.ylabel('Average Opinion')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        lstDATONIC = []
        n = len(tutto()[1][0])
        for i in range(n + 0):
            lstDATONIC.append(i)
        minimo = []
        massimo = []
        media = []
        mediana = []
        for l in np.array(lstDATONIC):
            minimo.append(dat[:, l].min())
            massimo.append(dat[:, l].max())
            media.append(dat[:, l].mean())
            mediana.append(np.median(dat[:, l], axis=0))
        plt.plot(minimo[1:], label='minimum')
        plt.plot(massimo[1:], label='maximum')
        plt.plot(media[1:], label='mean')
        plt.plot(mediana[1:], 'r--', label='median')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.title('step 3: introducing differences between levels')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()

        dat = np.array(DATONID)
        for n in dat:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 4: introducing differences between Simmelian and non simmelian (sigma)')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datcumulativeD = np.array(cumulativeD)
        for n in datcumulativeD:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 4: introducing differences between Simmelian and non simmelian (sigma)')
        plt.xlabel('Time steps')
        plt.ylabel('Cumulative adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datopinionD = np.array(opinionD)
        for n in datopinionD:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 4: introducing differences between Simmelian and non simmelian (sigma)')
        plt.xlabel('Time steps')
        plt.ylabel('Average Opinion')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        lstDATONID = []
        n = len(tutto()[1][0])
        for i in range(n + 0):
            lstDATONID.append(i)
        minimo = []
        massimo = []
        media = []
        mediana = []
        for l in np.array(lstDATONID):
            minimo.append(dat[:, l].min())
            massimo.append(dat[:, l].max())
            media.append(dat[:, l].mean())
            mediana.append(np.median(dat[:, l], axis=0))
        plt.plot(minimo[1:], label='minimum')
        plt.plot(massimo[1:], label='maximum')
        plt.plot(media[1:], label='mean')
        plt.plot(mediana[1:], 'r--', label='median')
        plt.title('step 4: introducing differences between Simmelian and non simmelian (sigma)')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()

        dat = np.array(DATONIE)
        for n in dat:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 5: introducing differences between Simmelian and non simmelian (sigma and mean)')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datcumulativeE = np.array(cumulativeE)
        for n in datcumulativeE:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 5: introducing differences between Simmelian and non simmelian (sigma and mean)')
        plt.xlabel('Time steps')
        plt.ylabel('Cumulative adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datopinionE = np.array(opinionE)
        for n in datopinionE:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 5: introducing differences between Simmelian and non simmelian (sigma and mean')
        plt.xlabel('Time steps')
        plt.ylabel('Average Opinion')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        lstDATONIE = []
        n = len(tutto()[1][0])
        for i in range(n + 0):
            lstDATONIE.append(i)
        minimo = []
        massimo = []
        media = []
        mediana = []
        for l in np.array(lstDATONIE):
            minimo.append(dat[:, l].min())
            massimo.append(dat[:, l].max())
            media.append(dat[:, l].mean())
            mediana.append(np.median(dat[:, l], axis=0))
        plt.plot(minimo[1:], label='minimum')
        plt.plot(massimo[1:], label='maximum')
        plt.plot(media[1:], label='mean')
        plt.plot(mediana[1:], 'r--', label='median')
        plt.legend()
        plt.title('step 5: introducing differences between Simmelian and non simmelian (sigma and mean)')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()


def opinion_dynamics(num, par, steps, alpha, beta, thr, stub, primini):
        p = par
        F = nx.random_tree(n=num, create_using=nx.DiGraph, )
        G = nx.barabasi_albert_graph(num, p)
        nx.set_edge_attributes(G, 0, name='threshold')
        for n in G.nodes:
            G.nodes[n]['threshold'] = thr
        datoni = []

        def combine(arr, s):
            return list(combinations(arr, s))

        def Control_Centrality(F):
            length = len(F.nodes)
            i = 1

            def CC(F):
                pippa = F.out_degree()
                pippa1 = np.array(pippa)
                pippa2 = pippa1[pippa1[:, 1] != 0]
                F = F.subgraph(pippa2[:, 0])
                return F

            def layer(F):
                culo = [x for x in F.nodes if x not in CC(F).nodes]
                banana = [i] * len(culo)
                results = np.column_stack((culo, banana))
                return results

            results = layer(F)
            while len(results) < length:
                results = np.concatenate([results, layer(F)])
                results = np.unique(results, axis=0)
                i += 1
                F = CC(F)
                layer(F)
            return dict(zip(results[:, 0], (results[:, 1])))

        BC = nx.betweenness_centrality(F)

        isinstance(BC, dict)
        list(G.nodes)

        lll = combinations(G.nodes(), 2)
        lll = np.matrix(list(lll))
        cliques = list(nx.enumerate_all_cliques(G))
        triads = [i for i in cliques if len(i) == 3]
        simm = []
        for n in triads:
            for l in combine(n, 2):
                simm.append(l)

        simmelian = []
        for i in simm:
            if i not in simmelian:
                simmelian.append(i)

        simmelian = np.array(simmelian)

        A = simmelian[:, 0]
        B = simmelian[:, 1]
        A1 = lll[:, 0]
        B1 = lll[:, 1]
        df = pd.DataFrame(simmelian)
        numsimm = len(simmelian)

        BCA = []
        for x in A:
            for n in BC:
                if x == n:
                    BCA.append(BC[n])
        BCB = []
        for y in B:
            for n in BC:
                if y == n:
                    BCB.append(BC[n])

        df['BCA'] = BCA
        df['BCB'] = BCB
        df['AVG'] = (df['BCA'] + df['BCB']) / 2
        df['VAR'] = (((df['BCA'] - df['AVG']) ** 2 + (df['BCB'] - df['AVG']) ** 2) / 2)
        VAR = np.sum(df['VAR'])
        V = VAR / numsimm
        df1 = df[3:4]
        dff = []
        for k in G.nodes:
            for n in BC:
                if k == n:
                    dff.append(BC[n])
        T = max(dff)

        H = pd.DataFrame(dff)
        H['BCMax'] = T
        H.rename(columns={0: 'BCentrality'}, inplace=True)
        H['Hier'] = (H['BCMax'] - H['BCentrality'])
        Q = np.mean(H['Hier'])
        v = Q * V

        BC1 = nx.betweenness_centrality(F)
        isinstance(BC1, dict)
        df1 = pd.DataFrame(lll)
        BC1A = []
        for x in A1:
            for n in BC1:
                if x == n:
                    BC1A.append(BC1[n])
        BC1B = []
        for y in B1:
            for n in BC1:
                if y == n:
                    BC1B.append(BC1[n])
        df1['BC1A'] = BC1A
        df1['BC1B'] = BC1B
        df1['AVG1'] = (df1['BC1A'] + df1['BC1B']) / 2
        df1['VAR1'] = ((df1['BC1A'] - df1['AVG1']) ** 2 + (df1['BC1B'] - df1['AVG1']) ** 2) / 2
        V1 = np.mean(df1['VAR1'])
        dff1 = []
        for k in G.nodes:
            for n in BC1:
                if k == n:
                    dff1.append(BC1[n])
        T1 = max(dff1)
        H1 = pd.DataFrame(dff1)
        H1['BC1Max'] = T1
        H1.rename(columns={0: 'BCentrality1'}, inplace=True)
        H1['Hier1'] = (H1['BC1Max'] - H1['BCentrality1'])
        Vl = V / V1
        nx.set_edge_attributes(G, 0, name='imitation')
        for n in G.nodes:
            G.nodes[n]['imitation'] = randrange(1, 10) / 100

        sup = []
        for (i, j) in F.edges:
            sup.append((j, i))

        sub = F.edges

        nx.set_edge_attributes(G, 0, name='theta')
        for n in G.nodes:
            G.nodes[n]['theta'] = randrange(1, 10) / 500
        nx.set_edge_attributes(G, 0, name='ego')
        for n in G.nodes:
            G.nodes[n]['ego'] = randrange(4, 9) / 10
        nx.set_node_attributes(G, 0, name='relevance')
        for n in G.nodes:
            G.nodes[n]['relevance'] = np.random.beta(alpha, beta)
        stubborn = random.sample(range(0, num), stub)
        primi = random.sample(range(0, num), primini)

        for n in primi:
            G.nodes[n]['relevance'] = 1
        for n in stubborn:
            G.nodes[n]['relevance'] = randrange(0,100)/100

        banana = random.sample(range(0, num), stub)
        print(banana)

        influence_matrix = np.matrix(np.zeros((num, num)))
        for i in G.nodes:
            for j in G.neighbors(i):
                if i in stubborn:
                    influence_matrix[i, j] = 0
                else:
                    if BC[i] - BC[j] < 0:
                        influence_matrix[i, j] = (np.random.beta(1, 8) / len(list(G.neighbors(i))))
                    elif BC[i] - BC[j] > 0:
                        influence_matrix[i, j] = np.random.beta(1, 2) / len(list(G.neighbors(i)))
                    elif BC[i] - BC[j] == 0:
                        influence_matrix[i, j] = np.random.beta(4, 3) / len(list(G.neighbors(i)))
            influence_matrix[i, i] = (1 - influence_matrix[[i]].sum(axis=1))



        probability_matrix = np.matrix(np.zeros((num, num)))
        for i in G.nodes:
            for j in G.neighbors(i):
                if i == j:
                    probability_matrix[i, j] = 1
                elif i != j:
                    if [i, j] in simmelian:
                        if BC[i] - BC[j] > 0:
                            probability_matrix[i, j] = np.random.beta(8, 3)
                        elif BC[i] - BC[j] < 0:
                            probability_matrix[i, j] = np.random.beta(8, 2)
                        elif BC[i] - BC[j] == 0:
                            probability_matrix[i, j] = np.random.beta(8, 1)
                    elif [i, j] not in simmelian:
                        if BC[i] - BC[j] > 0:
                            probability_matrix[i, j] = np.random.beta(8, 4)
                        elif BC[i] - BC[j] < 0:
                            probability_matrix[i, j] = np.random.beta(8, 4)
                        elif BC[i] - BC[j] == 0:
                            probability_matrix[i, j] = np.random.beta(8, 4)

        bellini = []
        def degrootmodified(G):
            K = G.copy()
            t = 0

            def event():
                P = K.copy()
                for i in K.nodes:
                    opchange = [P.nodes[i]['relevance'] * influence_matrix[i, i]]
                    for j in K.neighbors(i):
                        if P.nodes[j]['relevance'] >= P.nodes[j]['threshold']:
                                opchange.append(influence_matrix[i, j] * P.nodes[j]['relevance'])
                        elif P.nodes[j]['relevance'] < P.nodes[j]['threshold']:
                            opchange.append(influence_matrix[i, j] * P.nodes[i]['relevance'])
                    K.nodes[i]["relevance"] = np.sum(opchange)
                    if K.nodes[i]["relevance"] > 1:
                        K.nodes[i]["relevance"] = 1

                return K
            cristo = [(primini/num)]
            while t != steps:
                datelli = []
                datini = []
                t += 1
                event()
                for n in K.nodes:
                    datelli.append(K.nodes[n]['relevance'])
                    if K.nodes[n]['relevance'] > K.nodes[n]['threshold']:
                        datini.append(n)
                pd.DataFrame({'x': datelli})
                datoni.append(datelli)
                cristo.append(len(datini)/num)

            plt.plot(datoni, 'k', alpha=0.1)
            plt.plot(cristo, 'green',alpha=1, label = 'adoption rate')
            plt.plot(np.mean(datoni,axis=1), 'r--', label='mean')
            plt.plot(np.median(datoni,axis=1),'b', label='median')
            plt.axhline(y = thr, color = 'purple', label = 'threshold (avg)')
            plt.legend()
            plt.grid()
            plt.show()



        def degrootmodifiedstoc(G):
            K = G.copy()
            t = 0

            def event():
                P = K.copy()
                for i in K.nodes:
                    opchange = [P.nodes[i]['relevance'] * influence_matrix[i, i]]
                    for j in K.neighbors(i):
                        if P.nodes[j]['relevance'] >= P.nodes[j]['threshold']:
                            if random.random() < probability_matrix[i, j]:
                                opchange.append(influence_matrix[i, j] * P.nodes[j]['relevance'])
                            else:
                                opchange.append(influence_matrix[i, j] * P.nodes[i]['relevance'])
                        elif P.nodes[j]['relevance'] < P.nodes[j]['threshold']:
                            opchange.append(influence_matrix[i, j] * P.nodes[i]['relevance'])
                    K.nodes[i]["relevance"] = np.sum(opchange)
                    if K.nodes[i]["relevance"] > 1:
                        K.nodes[i]["relevance"] = 1

                return K
            cristo = [(primini / num)]
            while t != steps:
                datelli = []
                datini = []
                t += 1
                event()
                for n in K.nodes:
                    datelli.append(K.nodes[n]['relevance'])
                    if K.nodes[n]['relevance'] > K.nodes[n]['threshold']:
                        datini.append(n)
                pd.DataFrame({'x': datelli})
                datoni.append(datelli)
                cristo.append(len(datini) / num)

            plt.plot(datoni, 'k', alpha=0.1)
            plt.plot(cristo, 'green', alpha=1, label = 'adoption rate')
            plt.plot(np.mean(datoni, axis=1), 'r--', label='mean')
            plt.plot(np.median(datoni, axis=1), 'b', label='median')
            plt.axhline(y = thr, color = 'purple', label = 'threshold (avg)')
            plt.legend()
            plt.grid()
            plt.show()
        def degroottilde(G):
            K = G.copy()
            t = 0

            def event():
                P = K.copy()
                for i in K.nodes:
                    opchange = [P.nodes[i]['relevance'] * influence_matrix[i, i]]
                    for j in K.neighbors(i):
                        if P.nodes[j]['relevance'] >= P.nodes[j]['threshold']:
                                opchange.append(influence_matrix[i, j] * P.nodes[j]['relevance']*probability_matrix[i, j])
                        elif P.nodes[j]['relevance'] < P.nodes[j]['threshold']:
                            opchange.append(influence_matrix[i, j] * P.nodes[i]['relevance'])
                    K.nodes[i]["relevance"] = np.sum(opchange)
                    if K.nodes[i]["relevance"] > 1:
                        K.nodes[i]["relevance"] = 1

                return K
            cristo = [(primini / num)]
            while t != steps:
                datelli = []
                datini = []
                t += 1
                event()
                for n in K.nodes:
                    datelli.append(K.nodes[n]['relevance'])
                    if K.nodes[n]['relevance'] > K.nodes[n]['threshold']:
                        datini.append(n)
                pd.DataFrame({'x': datelli})
                datoni.append(datelli)
                cristo.append(len(datini) / num)

            plt.plot(datoni, 'k', alpha=0.1)
            plt.plot(cristo, 'green', alpha=1, label = 'adoption rate')
            plt.plot(np.mean(datoni, axis=1), 'r--', label='mean')
            plt.plot(np.median(datoni, axis=1), 'b', label='median')
            plt.axhline(y = thr, color = 'purple', label = 'threshold (avg)')
            plt.legend()
            plt.grid()
            plt.show()
        def degroot(G):
            K = G.copy()
            t = 0

            def event():
                P = K.copy()
                for i in K.nodes:
                    opchange = [P.nodes[i]['relevance'] * influence_matrix[i, i]]
                    for j in K.neighbors(i):
                        if random.random() < probability_matrix[i, j]:
                                opchange.append(P.nodes[j]['relevance'] * influence_matrix[i, j])
                        else:
                            opchange.append(P.nodes[i]['relevance'] * influence_matrix[i, j])
                    K.nodes[i]["relevance"] = np.sum(opchange)
                return K

            while t != steps:
                datelli = []
                t += 1
                event()
                for n in K.nodes:
                    datelli.append(K.nodes[n]['relevance'])
                pd.DataFrame({'x': datelli})
                datoni.append(datelli)
            plt.plot(datoni, 'k', alpha=0.1)
            mean = []
            plt.plot(np.mean(datoni, axis=1), 'r--', label='mean')
            plt.plot(np.median(datoni, axis=1), 'b', label='median')
            plt.axhline(y = thr, color = 'purple', label = 'threshold (avg)')
            plt.legend()
            plt.grid()
            plt.show()




        degroot(G)
        datoni = []
        degrootmodified(G)
        datoni = []
        degrootmodifiedstoc(G)
        datoni = []
        degroottilde(G)
        
        
def seeding(num, par, steps, iters, thr, primini):
        m = 0

        p = par

        def tutto():
            F = nx.random_tree(n=num, create_using=nx.DiGraph, )
            G = nx.barabasi_albert_graph(num, p)
            nx.set_node_attributes(G, 0, name='relevance')
            primi = random.sample(range(0, num), primini)
            for n in primi:
                G.nodes[n]['relevance'] = 1

            def combine(arr, s):
                return list(combinations(arr, s))

            def Control_Centrality(F):
                length = len(F.nodes)
                i = 1

                def CC(F):
                    pippa = F.out_degree()
                    pippa1 = np.array(pippa)
                    pippa2 = pippa1[pippa1[:, 1] != 0]
                    F = F.subgraph(pippa2[:, 0])
                    return F

                def layer(F):
                    culo = [x for x in F.nodes if x not in CC(F).nodes]
                    banana = [i] * len(culo)
                    results = np.column_stack((culo, banana))
                    return results

                results = layer(F)
                while len(results) < length:
                    results = np.concatenate([results, layer(F)])
                    results = np.unique(results, axis=0)
                    i += 1
                    F = CC(F)
                    layer(F)
                return dict(zip(results[:, 0], (results[:, 1])))

            BC = nx.betweenness_centrality(F)

            isinstance(BC, dict)
            list(G.nodes)

            lll = combinations(G.nodes(), 2)
            lll = np.matrix(list(lll))
            cliques = list(nx.enumerate_all_cliques(G))
            triads = [i for i in cliques if len(i) == 3]
            simm = []
            for n in triads:
                for l in combine(n, 2):
                    simm.append(l)

            simmelian = []
            for i in simm:
                if i not in simmelian:
                    simmelian.append(i)

            simmelian = np.array(simmelian)

            A = simmelian[:, 0]
            B = simmelian[:, 1]
            A1 = lll[:, 0]
            B1 = lll[:, 1]
            df = pd.DataFrame(simmelian)
            numsimm = len(simmelian)

            BCA = []
            for x in A:
                for n in BC:
                    if x == n:
                        BCA.append(BC[n])
            BCB = []
            for y in B:
                for n in BC:
                    if y == n:
                        BCB.append(BC[n])

            df['BCA'] = BCA
            df['BCB'] = BCB
            df['AVG'] = (df['BCA'] + df['BCB']) / 2
            df['VAR'] = (((df['BCA'] - df['AVG']) ** 2 + (df['BCB'] - df['AVG']) ** 2) / 2)
            VAR = np.sum(df['VAR'])
            V = VAR / numsimm
            df1 = df[3:4]
            dff = []
            for k in G.nodes:
                for n in BC:
                    if k == n:
                        dff.append(BC[n])
            T = max(dff)

            H = pd.DataFrame(dff)
            H['BCMax'] = T
            H.rename(columns={0: 'BCentrality'}, inplace=True)
            H['Hier'] = (H['BCMax'] - H['BCentrality'])
            Q = np.mean(H['Hier'])
            v = Q * V

            BC1 = nx.betweenness_centrality(F)
            isinstance(BC1, dict)
            df1 = pd.DataFrame(lll)
            BC1A = []
            for x in A1:
                for n in BC1:
                    if x == n:
                        BC1A.append(BC1[n])
            BC1B = []
            for y in B1:
                for n in BC1:
                    if y == n:
                        BC1B.append(BC1[n])
            df1['BC1A'] = BC1A
            df1['BC1B'] = BC1B
            df1['AVG1'] = (df1['BC1A'] + df1['BC1B']) / 2
            df1['VAR1'] = ((df1['BC1A'] - df1['AVG1']) ** 2 + (df1['BC1B'] - df1['AVG1']) ** 2) / 2
            V1 = np.mean(df1['VAR1'])
            dff1 = []
            for k in G.nodes:
                for n in BC1:
                    if k == n:
                        dff1.append(BC1[n])
            T1 = max(dff1)
            H1 = pd.DataFrame(dff1)
            H1['BC1Max'] = T1
            H1.rename(columns={0: 'BCentrality1'}, inplace=True)
            H1['Hier1'] = (H1['BC1Max'] - H1['BCentrality1'])
            Vl = V / V1
            nx.set_edge_attributes(G, 0, name='imitation')
            for n in G.nodes:
                G.nodes[n]['imitation'] = randrange(1, 10) / 100

            sup = []
            for (i, j) in F.edges:
                sup.append((j, i))

            sub = F.edges

            nx.set_edge_attributes(G, 0, name='theta')
            for n in G.nodes:
                G.nodes[n]['theta'] = randrange(1, 10) / 500
            nx.set_edge_attributes(G, 0, name='ego')
            for n in G.nodes:
                G.nodes[n]['ego'] = randrange(4, 9) / 10

            def modelling(meansup, meansequal, meansdown, meannup, meannequal, meanndown, sigmas, sigman,
                          ):
                """
                sup is when the information is going upward, down when it goes downward. if the information is going
                upword, it means that subject i is the superior


                for mean 0.8 is max and 0.2 minimum
                for sigma 0.05 is max and 0.01 is minimum
                """
                influence_matrix = np.matrix(np.zeros((num, num)))
                for i in G.nodes:
                    for j in G.nodes:
                        if i != j:
                            if j in G.neighbors(i):
                                if BC[i] - BC[j] < 0:
                                    influence_matrix[i, j] = np.random.normal(meanndown, sigman) / len(
                                        list(G.neighbors(i)))
                                elif BC[i] - BC[j] > 0:
                                    influence_matrix[i, j] = np.random.normal(meannup, sigman) / len(
                                        list(G.neighbors(i)))
                                elif BC[i] - BC[j] == 0:
                                    influence_matrix[i, j] = np.random.normal(meannequal, sigman) / len(
                                        list(G.neighbors(i)))
                        if influence_matrix[i, j] < 0:
                            influence_matrix[i, j] = 0
                        elif influence_matrix[i, j] > 1:
                            influence_matrix[i, j] = 1
                    influence_matrix[i, i] = (1 - influence_matrix[[i]].sum(axis=1))

                probability_matrix = np.matrix(np.zeros((num, num)))
                for i in G.nodes:
                    for j in G.nodes:
                        if i == j:
                            probability_matrix[i, j] = 1
                        elif i != j:
                            if [i, j] in simmelian:
                                if BC[i] - BC[j] > 0:
                                    probability_matrix[i, j] = np.random.normal(meansup, sigmas)
                                elif BC[i] - BC[j] < 0:
                                    probability_matrix[i, j] = np.random.normal(meansdown, sigmas)
                                elif BC[i] - BC[j] == 0:
                                    probability_matrix[i, j] = np.random.normal(meansequal, sigmas)
                            elif [i, j] not in simmelian:
                                if BC[i] - BC[j] > 0:
                                    probability_matrix[i, j] = np.random.normal(meannup, sigman)
                                elif BC[i] - BC[j] < 0:
                                    probability_matrix[i, j] = np.random.normal(meanndown, sigman)
                                elif BC[i] - BC[j] == 0:
                                    probability_matrix[i, j] = np.random.normal(meannequal, sigman)
                        if probability_matrix[i, j] < 0:
                            probability_matrix[i, j] = 0
                        elif probability_matrix[i, j] > 1:
                            probability_matrix[i, j] = 1

                nx.set_edge_attributes(G, 0, name='threshold')
                for n in G.nodes:
                    G.nodes[n]['threshold'] = thr

                K = G.copy()
                t = 0
                bellini = []

                def event():
                    P = K.copy()
                    for i in K.nodes:
                        opchange = [P.nodes[i]['relevance'] * influence_matrix[i, i]]
                        for j in K.neighbors(i):
                            if P.nodes[j]['relevance'] >= P.nodes[j]['threshold']:
                                if random.random() < probability_matrix[i, j]:
                                    opchange.append(influence_matrix[i, j] * P.nodes[j]['relevance'])
                                else:
                                    opchange.append(influence_matrix[i, j] * P.nodes[i]['relevance'])
                            if P.nodes[j]['relevance'] < P.nodes[j]['threshold']:
                                opchange.append(influence_matrix[i, j] * P.nodes[i]['relevance'])
                        K.nodes[i]["relevance"] = np.sum(opchange)
                        if K.nodes[i]["relevance"] > 1:
                            K.nodes[i]["relevance"] = 1

                    return K

                dati = [Vl, (primini / num)]
                datelli = [(primini / num)]
                datini = [0]
                while t != steps:

                    event()
                    P = []
                    P2 = []
                    for n in K.nodes:
                        if K.nodes[n]['relevance'] >= K.nodes[n]['threshold']:
                            P.append(K.nodes[n]['relevance'])
                            if n not in bellini:
                                bellini.append(n)
                        P2.append(K.nodes[n]['relevance'])
                    dati.append((len(P) / num))
                    datelli.append((len(bellini) / num))
                    datini.append(np.mean(P2))
                    t += 1
                if datini[-1]>thr*1.5:
                    print('Informal edges:',G.edges)
                    print('formal edges:', F.edges)
                    print('probability matrix:',repr(probability_matrix))
                    print('influence matrix:',repr(influence_matrix))
                    print('Primi:',print(primi))
                
                return [dati, datelli, datini]
            

            """
                    sup is when the information is going upward, down when it goes downward. if the information is going
                    upword, it means that subject i is the superior


                    for mean 0.8 is max and 0.2 minimum
                    for sigma 0.05 is max and 0.01 is minimum
                    """

            return [modelling(meansup=0.1, meansequal=0.2, meansdown=0.3, meannup=0.2, meannequal=0.4, meanndown=0.6,
                              sigmas=0.01, sigman=0.05), Vl]

        """
        the first one is with no thresholod
        the second one is with average threshold
        the third one includes differences between levels
        the fourth one considers divergencies between simmelian and non simmelian
        """


        DATONIE = []
        DATONIphi = []
        cumulativeE = []
        opinionE = []
        startTime = time.time()

        while m != iters:
            banana = tutto()
            executiontime = (time.time() - startTime)
            ETA = int((((executiontime / (m + 1)) * (iters - m))) / 60)
            print(int((m / iters) * 100), '% Estimated time remaining:',
                  ETA,
                  'minutes')
            m += 1

            DATONIE.append(banana[0][0])
            DATONIphi.append(banana[1])
            cumulativeE.append((banana[0][1]))
            opinionE.append((banana[0][2]))
            


        dat = np.array(DATONIE)
        for n in dat:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 5: introducing differences between Simmelian and non simmelian (sigma and mean)')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datcumulativeE = np.array(cumulativeE)
        for n in datcumulativeE:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 5: introducing differences between Simmelian and non simmelian (sigma and mean)')
        plt.xlabel('Time steps')
        plt.ylabel('Cumulative adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datopinionE = np.array(opinionE)
        for n in datopinionE:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 5: introducing differences between Simmelian and non simmelian (sigma and mean')
        plt.xlabel('Time steps')
        plt.ylabel('Average Opinion')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        lstDATONIE = []
        n = len(tutto()[0][0])
        for i in range(n + 0):
            lstDATONIE.append(i)
        minimo = []
        massimo = []
        media = []
        mediana = []
        for l in np.array(lstDATONIE):
            minimo.append(dat[:, l].min())
            massimo.append(dat[:, l].max())
            media.append(dat[:, l].mean())
            mediana.append(np.median(dat[:, l], axis=0))
        plt.plot(minimo[1:], label='minimum')
        plt.plot(massimo[1:], label='maximum')
        plt.plot(media[1:], label='mean')
        plt.plot(mediana[1:], 'r--', label='median')
        plt.legend()
        plt.title('step 5: introducing differences between Simmelian and non simmelian (sigma and mean)')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()



def parameter_analysis_with_seed(steps, iters, thr):
        thr = thr
        m = 0
        primi = 6
        F = nx.DiGraph()
        G = nx.Graph()
        G.add_edges_from([(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 10), (0, 13), (0, 17), (0, 18), (1, 5), (1, 7), (1, 8), (1, 10), (1, 11), (1, 13), (1, 14), (1, 15), (1, 16), (2, 8), (2, 15), (2, 16), (2, 18), (3, 5), (3, 6), (3, 9), (3, 12), (3, 14), (4, 5), (4, 6), (4, 9), (4, 10), (4, 12), (4, 15), (4, 17), (5, 6), (5, 7), (5, 10), (5, 13), (5, 15), (6, 7), (6, 8), (6, 9), (6, 16), (7, 8), (7, 11), (7, 12), (7, 13), (7, 19), (8, 9), (8, 11), (9, 11), (9, 12), (9, 14), (9, 19), (10, 14), (11, 16), (11, 17), (11, 18), (12, 18), (13, 19), (15, 17), (17, 19)])
        F.add_edges_from([(0, 19), (1, 12), (1, 16), (3, 7), (4, 18), (6, 3), (6, 15), (7, 9), (7, 17), (8, 14), (10, 2), (11, 6), (14, 13), (15, 4), (17, 5), (18, 10), (18, 1), (19, 8), (19, 11)])
        num=len(list(G.nodes))
        influenceseed = np.matrix([[0.45331285, 0.06112131, 0.0341564 , 0.05129056, 0.05374631,
         0.03966023, 0.04751791, 0.05887164, 0.        , 0.        ,
         0.05296844, 0.        , 0.        , 0.03836455, 0.        ,
         0.        , 0.        , 0.05144483, 0.05754498, 0.        ],
        [0.01476535, 0.68712466, 0.        , 0.        , 0.        ,
         0.01282206, 0.        , 0.05809783, 0.01688056, 0.        ,
         0.02218068, 0.0639313 , 0.        , 0.01996131, 0.01975889,
         0.05723898, 0.02723838, 0.        , 0.        , 0.        ],
        [0.06334699, 0.        , 0.45340633, 0.        , 0.        ,
         0.        , 0.        , 0.        , 0.12948745, 0.        ,
         0.        , 0.        , 0.        , 0.        , 0.        ,
         0.13189903, 0.09804367, 0.        , 0.12381654, 0.        ],
        [0.0218249 , 0.        , 0.        , 0.73520912, 0.        ,
         0.05232826, 0.09474024, 0.        , 0.        , 0.02594861,
         0.        , 0.        , 0.03250727, 0.        , 0.0374416 ,
         0.        , 0.        , 0.        , 0.        , 0.        ],
        [0.01770802, 0.        , 0.        , 0.        , 0.72943857,
         0.02750922, 0.08518808, 0.        , 0.        , 0.02803012,
         0.02272411, 0.        , 0.03481427, 0.        , 0.        ,
         0.02597107, 0.        , 0.02861656, 0.        , 0.        ],
        [0.04263786, 0.0592024 , 0.        , 0.05971554, 0.07102396,
         0.4341042 , 0.07085848, 0.07322865, 0.        , 0.        ,
         0.06885839, 0.        , 0.        , 0.05069445, 0.        ,
         0.06967605, 0.        , 0.        , 0.        , 0.        ],
        [0.01763336, 0.        , 0.        , 0.0236826 , 0.02614272,
         0.0146319 , 0.84445731, 0.0130585 , 0.01536075, 0.01490867,
         0.        , 0.        , 0.        , 0.        , 0.        ,
         0.        , 0.03012418, 0.        , 0.        , 0.        ],
        [0.02492756, 0.01764814, 0.        , 0.        , 0.        ,
         0.02145405, 0.05999053, 0.67681637, 0.01520415, 0.        ,
         0.        , 0.06275234, 0.02751547, 0.02322802, 0.        ,
         0.        , 0.        , 0.        , 0.        , 0.07046336],
        [0.        , 0.08860973, 0.02370828, 0.        , 0.        ,
         0.        , 0.10087701, 0.10406229, 0.54198545, 0.02958216,
         0.        , 0.11117508, 0.        , 0.        , 0.        ,
         0.        , 0.        , 0.        , 0.        , 0.        ],
        [0.        , 0.        , 0.        , 0.06853927, 0.06964915,
         0.        , 0.07247483, 0.        , 0.07583402, 0.410837  ,
         0.        , 0.08430471, 0.05775445, 0.        , 0.08751763,
         0.        , 0.        , 0.        , 0.        , 0.07308893],
        [0.05163476, 0.1239248 , 0.        , 0.        , 0.12942087,
         0.04862065, 0.        , 0.        , 0.        , 0.        ,
         0.59306424, 0.        , 0.        , 0.        , 0.05333468,
         0.        , 0.        , 0.        , 0.        , 0.        ],
        [0.        , 0.02122541, 0.        , 0.        , 0.        ,
         0.        , 0.        , 0.02172751, 0.02209739, 0.02995534,
         0.        , 0.77241345, 0.        , 0.        , 0.        ,
         0.        , 0.03242806, 0.01659524, 0.0835576 , 0.        ],
        [0.        , 0.        , 0.        , 0.12463538, 0.11954605,
         0.        , 0.        , 0.1193657 , 0.        , 0.07684262,
         0.        , 0.        , 0.44031944, 0.        , 0.        ,
         0.        , 0.        , 0.        , 0.1192908 , 0.        ],
        [0.080072  , 0.11197049, 0.        , 0.        , 0.        ,
         0.08416621, 0.        , 0.1291707 , 0.        , 0.        ,
         0.        , 0.        , 0.        , 0.4871421 , 0.        ,
         0.        , 0.        , 0.        , 0.        , 0.10747851],
        [0.        , 0.1579823 , 0.        , 0.13706238, 0.        ,
         0.        , 0.        , 0.        , 0.        , 0.06075203,
         0.15505673, 0.        , 0.        , 0.        , 0.48914656,
         0.        , 0.        , 0.        , 0.        , 0.        ],
        [0.        , 0.0345856 , 0.035397  , 0.        , 0.10331707,
         0.03929356, 0.        , 0.        , 0.        , 0.        ,
         0.        , 0.        , 0.        , 0.        , 0.        ,
         0.7617187 , 0.        , 0.02568806, 0.        , 0.        ],
        [0.        , 0.17430087, 0.1057813 , 0.        , 0.        ,
         0.        , 0.15994573, 0.        , 0.        , 0.        ,
         0.        , 0.15084985, 0.        , 0.        , 0.        ,
         0.        , 0.40912225, 0.        , 0.        , 0.        ],
        [0.03850938, 0.        , 0.        , 0.        , 0.12262736,
         0.        , 0.        , 0.        , 0.        , 0.        ,
         0.        , 0.12051164, 0.        , 0.        , 0.        ,
         0.09818983, 0.        , 0.50908894, 0.        , 0.11107284],
        [0.03826476, 0.        , 0.03973528, 0.        , 0.        ,
         0.        , 0.        , 0.        , 0.        , 0.        ,
         0.        , 0.06086265, 0.06916252, 0.        , 0.        ,
         0.        , 0.        , 0.        , 0.79197479, 0.        ],
        [0.        , 0.        , 0.        , 0.        , 0.        ,
         0.        , 0.        , 0.06073204, 0.        , 0.0679092 ,
         0.        , 0.        , 0.        , 0.05554884, 0.        ,
         0.        , 0.        , 0.05118836, 0.        , 0.76462156]])
        probabilityseed = np.matrix([[1.        , 0.30576862, 0.20068752, 0.2959892 , 0.29783081,
         0.20340599, 0.30324671, 0.29681197, 0.29994118, 0.22126058,
         0.32703321, 0.29406252, 0.19915951, 0.19179506, 0.2967641 ,
         0.30954403, 0.20691822, 0.30001888, 0.30876291, 0.29412973],
        [0.11168464, 1.        , 0.10084913, 0.28523557, 0.29948487,
         0.10977166, 0.28422005, 0.28961667, 0.08728565, 0.09025829,
         0.10619355, 0.28616237, 0.08725238, 0.11225226, 0.09847633,
         0.29998654, 0.08747244, 0.11129446, 0.30334078, 0.30185219],
        [0.20986568, 0.29502606, 1.        , 0.28748747, 0.30640672,
         0.21212691, 0.29999926, 0.29052851, 0.28502474, 0.21130918,
         0.29760297, 0.28453711, 0.20787864, 0.1945703 , 0.28043312,
         0.31524195, 0.20193415, 0.28910322, 0.3019788 , 0.31956991],
        [0.09459274, 0.09175406, 0.10914551, 1.        , 0.29796913,
         0.11422677, 0.2786899 , 0.09855364, 0.10132257, 0.1123232 ,
         0.08654992, 0.30885713, 0.10491555, 0.10561383, 0.08063899,
         0.3089554 , 0.11967756, 0.1161078 , 0.30039256, 0.29490153],
        [0.12464198, 0.09637416, 0.10467316, 0.10170918, 1.        ,
         0.09918887, 0.28385934, 0.0855767 , 0.09713733, 0.09764816,
         0.10250938, 0.1207825 , 0.09713184, 0.1230613 , 0.0927758 ,
         0.10626634, 0.07604134, 0.10811175, 0.20334996, 0.10411108],
        [0.17886609, 0.29235233, 0.20468043, 0.29624091, 0.32340945,
         1.        , 0.30306362, 0.30439558, 0.28801299, 0.20097823,
         0.30339788, 0.27900794, 0.19262782, 0.1912987 , 0.30571362,
         0.3008892 , 0.20913239, 0.29809244, 0.30228362, 0.31252628],
        [0.08843184, 0.10756408, 0.09653427, 0.12131276, 0.10288746,
         0.08758143, 1.        , 0.09899517, 0.11413145, 0.08994253,
         0.09388033, 0.10757174, 0.09981224, 0.11261308, 0.11084461,
         0.10091417, 0.10265368, 0.09219389, 0.1061628 , 0.10684418],
        [0.08062852, 0.10640728, 0.09096941, 0.30363884, 0.30240184,
         0.09915372, 0.30346384, 1.        , 0.11266309, 0.08517394,
         0.09987499, 0.30407762, 0.10725316, 0.08992643, 0.10412757,
         0.29777883, 0.0947422 , 0.09832026, 0.29106397, 0.31572894],
        [0.1092765 , 0.28299234, 0.09098196, 0.30506615, 0.29515533,
         0.09988897, 0.3187452 , 0.28583956, 1.        , 0.10024666,
         0.29941558, 0.29562815, 0.09876455, 0.10294168, 0.10320906,
         0.2996555 , 0.09342413, 0.30352542, 0.29142218, 0.3078516 ],
        [0.19261447, 0.31598731, 0.2046894 , 0.29503359, 0.28724652,
         0.21342024, 0.31524487, 0.31293241, 0.29998066, 1.        ,
         0.30187682, 0.29816865, 0.18455956, 0.19070435, 0.2953268 ,
         0.31482926, 0.20618817, 0.31271421, 0.29151957, 0.27959118],
        [0.08565564, 0.31598791, 0.10762963, 0.29682634, 0.28059973,
         0.09642062, 0.30777005, 0.30913786, 0.09509164, 0.08122867,
         1.        , 0.29774717, 0.09109087, 0.10182619, 0.09827805,
         0.31756923, 0.09662328, 0.10359845, 0.30302999, 0.29386834],
        [0.10299015, 0.11199713, 0.10565569, 0.09003511, 0.30270458,
         0.10339192, 0.29070778, 0.08743586, 0.11276063, 0.09943876,
         0.09326394, 1.        , 0.08965342, 0.10151617, 0.09374697,
         0.2112933 , 0.11144622, 0.10089299, 0.2984205 , 0.11025814],
        [0.34239847, 0.30234341, 0.21616872, 0.30010139, 0.2720581 ,
         0.19767112, 0.30409675, 0.28381909, 0.29401671, 0.18465033,
         0.30928612, 0.29917156, 1.        , 0.18332241, 0.30657659,
         0.29625565, 0.19577707, 0.2945437 , 0.30498305, 0.31010372],
        [0.19838248, 0.30240936, 0.20322147, 0.29696128, 0.32156839,
         0.19246095, 0.30974327, 0.31366131, 0.29551458, 0.20914419,
         0.29527263, 0.29340292, 0.20413627, 1.        , 0.29970609,
         0.30746805, 0.21031331, 0.30612381, 0.28987363, 0.30323746],
        [0.2255411 , 0.28855276, 0.10176994, 0.28439361, 0.31892202,
         0.09459359, 0.31312078, 0.29789528, 0.28849576, 0.10057169,
         0.30866742, 0.30598261, 0.10181364, 0.09686915, 1.        ,
         0.29702976, 0.09353413, 0.30201138, 0.29416742, 0.28399078],
        [0.10251485, 0.10951821, 0.11386457, 0.0914028 , 0.28782722,
         0.10642254, 0.31127988, 0.10818601, 0.09690312, 0.10055536,
         0.1235367 , 0.19014611, 0.10249228, 0.09967273, 0.12752893,
         1.        , 0.11187627, 0.08943089, 0.31446372, 0.08831249],
        [0.34979012, 0.30865975, 0.17885057, 0.30941712, 0.30960037,
         0.22482785, 0.30562965, 0.27278259, 0.29902641, 0.21106318,
         0.28172096, 0.28604628, 0.20213426, 0.20867902, 0.30416113,
         0.29737441, 1.        , 0.30333683, 0.30912948, 0.29951255],
        [0.16953374, 0.30409693, 0.09499338, 0.30581111, 0.29958365,
         0.09775719, 0.29018398, 0.3092911 , 0.10387624, 0.07275083,
         0.31424886, 0.30495205, 0.10475387, 0.11079568, 0.1087682 ,
         0.29378542, 0.11123809, 1.        , 0.29740514, 0.28026462],
        [0.11523819, 0.07958993, 0.10987288, 0.0983797 , 0.19171006,
         0.10836589, 0.28640163, 0.09680617, 0.11013816, 0.11653968,
         0.10777083, 0.10494645, 0.11469263, 0.10502039, 0.09868976,
         0.10548009, 0.09355163, 0.11088569, 1.        , 0.10623161],
        [0.19332857, 0.08932915, 0.09903028, 0.10457082, 0.29562557,
         0.10638758, 0.30477728, 0.0956255 , 0.10015795, 0.1230665 ,
         0.11073443, 0.2895842 , 0.10262853, 0.07661541, 0.07784058,
         0.30217162, 0.08452037, 0.10727855, 0.28874588, 1.        ]])
        def tutto():
            nx.set_node_attributes(G, 0, name='relevance')
            G.nodes[primi]['relevance'] = 1

            def combine(arr, s):
                return list(combinations(arr, s))

            def Control_Centrality(F):
                length = len(F.nodes)
                i = 1

                def CC(F):
                    pippa = F.out_degree()
                    pippa1 = np.array(pippa)
                    pippa2 = pippa1[pippa1[:, 1] != 0]
                    F = F.subgraph(pippa2[:, 0])
                    return F

                def layer(F):
                    culo = [x for x in F.nodes if x not in CC(F).nodes]
                    banana = [i] * len(culo)
                    results = np.column_stack((culo, banana))
                    return results

                results = layer(F)
                while len(results) < length:
                    results = np.concatenate([results, layer(F)])
                    results = np.unique(results, axis=0)
                    i += 1
                    F = CC(F)
                    layer(F)
                return dict(zip(results[:, 0], (results[:, 1])))

            BC = nx.betweenness_centrality(F)

            isinstance(BC, dict)
            list(G.nodes)

            lll = combinations(G.nodes(), 2)
            lll = np.matrix(list(lll))
            cliques = list(nx.enumerate_all_cliques(G))
            triads = [i for i in cliques if len(i) == 3]
            simm = []
            for n in triads:
                for l in combine(n, 2):
                    simm.append(l)

            simmelian = []
            for i in simm:
                if i not in simmelian:
                    simmelian.append(i)

            
            simmelian = np.array(simmelian)
            if simmelian.size < 1:
                simmelian = np.matrix([[0,0]])

            A = simmelian[:,0]
            B = simmelian[:,1]
            A1 = lll[:, 0]
            B1 = lll[:, 1]
            df = pd.DataFrame(simmelian)
            numsimm = len(simmelian)

            BCA = []
            for x in A:
                for n in BC:
                    if x == n:
                        BCA.append(BC[n])
            BCB = []
            for y in B:
                for n in BC:
                    if y == n:
                        BCB.append(BC[n])

            df['BCA'] = BCA
            df['BCB'] = BCB
            df['AVG'] = (df['BCA'] + df['BCB']) / 2
            df['VAR'] = (((df['BCA'] - df['AVG']) ** 2 + (df['BCB'] - df['AVG']) ** 2) / 2)
            VAR = np.sum(df['VAR'])
            V = VAR / numsimm
            df1 = df[3:4]
            dff = []
            for k in G.nodes:
                for n in BC:
                    if k == n:
                        dff.append(BC[n])
            T = max(dff)

            H = pd.DataFrame(dff)
            H['BCMax'] = T
            H.rename(columns={0: 'BCentrality'}, inplace=True)
            H['Hier'] = (H['BCMax'] - H['BCentrality'])
            Q = np.mean(H['Hier'])
            v = Q * V

            BC1 = nx.betweenness_centrality(F)
            isinstance(BC1, dict)
            df1 = pd.DataFrame(lll)
            BC1A = []
            for x in A1:
                for n in BC1:
                    if x == n:
                        BC1A.append(BC1[n])
            BC1B = []
            for y in B1:
                for n in BC1:
                    if y == n:
                        BC1B.append(BC1[n])
            df1['BC1A'] = BC1A
            df1['BC1B'] = BC1B
            df1['AVG1'] = (df1['BC1A'] + df1['BC1B']) / 2
            df1['VAR1'] = ((df1['BC1A'] - df1['AVG1']) ** 2 + (df1['BC1B'] - df1['AVG1']) ** 2) / 2
            V1 = np.mean(df1['VAR1'])
            dff1 = []
            for k in G.nodes:
                for n in BC1:
                    if k == n:
                        dff1.append(BC1[n])
            T1 = max(dff1)
            H1 = pd.DataFrame(dff1)
            H1['BC1Max'] = T1
            H1.rename(columns={0: 'BCentrality1'}, inplace=True)
            H1['Hier1'] = (H1['BC1Max'] - H1['BCentrality1'])
            Vl = V / V1
            sup = []
            for (i, j) in F.edges:
                sup.append((j, i))

            sub = F.edges

            nx.set_edge_attributes(G, 0, name='theta')
            for n in G.nodes:
                G.nodes[n]['theta'] = randrange(1, 10) / 500

            def modelling(meansup, meansequal, meansdown, meannup, meannequal, meanndown, sigmas, sigman,
                          ):
                """
                sup is when the information is going upward, down when it goes downward. if the information is going
                upword, it means that subject i is the superior


                for mean 0.8 is max and 0.2 minimum
                for sigma 0.05 is max and 0.01 is minimum
                """
                influence_matrix = influenceseed

                probability_matrix = probabilityseed

                nx.set_edge_attributes(G, 0, name='threshold')
                for n in G.nodes:
                    G.nodes[n]['threshold'] = thr


                K = G.copy()
                t = 0
                bellini = []

                def event():
                    P = K.copy()
                    for i in K.nodes:
                        opchange = [P.nodes[i]['relevance'] * influence_matrix[i, i]]
                        for j in K.neighbors(i):
                            if P.nodes[j]['relevance'] >= P.nodes[j]['threshold']:
                                if random.random() < probability_matrix[i, j]:
                                    opchange.append(influence_matrix[i, j] * P.nodes[j]['relevance'])
                                else:
                                    opchange.append(influence_matrix[i, j] * P.nodes[i]['relevance'])
                            if P.nodes[j]['relevance'] < P.nodes[j]['threshold']:
                                opchange.append(influence_matrix[i, j] * P.nodes[i]['relevance'])
                        K.nodes[i]["relevance"] = np.sum(opchange)
                        if K.nodes[i]["relevance"] > 1:
                            K.nodes[i]["relevance"] = 1

                    return K

                dati = [Vl, (1 / num)]
                datelli = [(1 / num)]
                datini = [0]
                while t != steps:

                    event()
                    P = []
                    P2 = []
                    for n in K.nodes:
                        if K.nodes[n]['relevance'] >= K.nodes[n]['threshold']:
                            P.append(K.nodes[n]['relevance'])
                            if n not in bellini:
                                bellini.append(n)
                        P2.append(K.nodes[n]['relevance'])
                    dati.append((len(P) / num))
                    datelli.append((len(bellini) / num))
                    datini.append(np.mean(P2))
                    t += 1

                return [dati, datelli, datini]

            """
                    sup is when the information is going upward, down when it goes downward. if the information is going
                    upword, it means that subject i is the superior


                    for mean 0.8 is max and 0.2 minimum
                    for sigma 0.05 is max and 0.01 is minimum
                    """

            return [modelling(meansup=1, meansequal=1, meansdown=1, meannup=1, meannequal=1, meanndown=1,
                              sigmas=0, sigman=0),
                    modelling(meansup=1, meansequal=1, meansdown=1, meannup=1, meannequal=1, meanndown=1,
                              sigmas=0, sigman=0),
                    modelling(meansup=0.0, meansequal=0.4, meansdown=0.6, meannup=0.2, meannequal=0.4, meanndown=0.6,

                              sigmas=0, sigman=0),
                    modelling(meansup=0.0, meansequal=0.4, meansdown=0.6, meannup=0.2, meannequal=0.4, meanndown=0.6,
                              sigmas=0.01, sigman=0.05),
                    modelling(meansup=0.2, meansequal=0.7, meansdown=0.8, meannup=0.2, meannequal=0.4, meanndown=0.6,
                              sigmas=0.01, sigman=0.05), Vl]

        """
        the first one is with no thresholod
        the second one is with average threshold
        the third one includes differences between levels
        the fourth one considers divergencies between simmelian and non simmelian
        """

        DATONIA = []
        DATONIB = []
        DATONIC = []
        DATONID = []
        DATONIE = []
        DATONIphi = []
        cumulativeA = []
        cumulativeB = []
        cumulativeC = []
        cumulativeD = []
        cumulativeE = []
        opinionA = []
        opinionB = []
        opinionC = []
        opinionD = []
        opinionE = []

        startTime = time.time()

        while m != iters:
            banana = tutto()
            executiontime = (time.time() - startTime)
            ETA = int((((executiontime / (m + 1)) * (iters - m))) / 60)
            print(int((m / iters) * 100), '% Estimated time remaining:',
                  ETA,
                  'minutes')
            m += 1
            DATONIA.append(banana[0][0])
            DATONIB.append(banana[1][0])
            DATONIC.append(banana[2][0])
            DATONID.append(banana[3][0])
            DATONIE.append(banana[4][0])
            DATONIphi.append(banana[5])
            cumulativeA.append((banana[0][1]))
            cumulativeB.append((banana[1][1]))
            cumulativeC.append((banana[2][1]))
            cumulativeD.append((banana[3][1]))
            cumulativeE.append((banana[4][1]))
            opinionA.append((banana[0][2]))
            opinionB.append((banana[1][2]))
            opinionC.append((banana[2][2]))
            opinionD.append((banana[3][2]))
            opinionE.append((banana[4][2]))

        dat = np.array(DATONIA)
        for n in dat:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 1: Every parameter is fixed')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datcumulativeA = np.array(cumulativeA)
        for n in datcumulativeA:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 1: Every parameter is fixed')
        plt.xlabel('Time steps')
        plt.ylabel('Cumulative adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datopinionA = np.array(opinionA)
        for n in datopinionA:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 1: Every parameter is fixed')
        plt.xlabel('Time steps')
        plt.ylabel('Average Opinion')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        lstDATONIA = []
        n = len(tutto()[1][0])
        for i in range(n + 0):
            lstDATONIA.append(i)
        minimo = []
        massimo = []
        media = []
        mediana = []
        for l in np.array(lstDATONIA):
            minimo.append(dat[:, l].min())
            massimo.append(dat[:, l].max())
            media.append(dat[:, l].mean())
            mediana.append(np.median(dat[:, l], axis=0))
        plt.plot(minimo[1:], label='minimum')
        plt.plot(massimo[1:], label='maximum')
        plt.plot(media[1:], label='mean')
        plt.plot(mediana[1:], 'r--', label='median')
        plt.legend()
        plt.title('step 1: Every parameter is fixed')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()

        dat = np.array(DATONIB)
        for n in dat:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 2: a (random) threshold is assigned')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datcumulativeB = np.array(cumulativeB)
        for n in datcumulativeB:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 2: a (random) threshold is assigned')
        plt.xlabel('Time steps')
        plt.ylabel('Cumulative adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datopinionB = np.array(opinionB)
        for n in datopinionB:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 2: a (random) threshold is assigned')
        plt.xlabel('Time steps')
        plt.ylabel('Average Opinion')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        lstDATONIB = []
        n = len(tutto()[1][0])
        for i in range(n + 0):
            lstDATONIB.append(i)
        minimo = []
        massimo = []
        media = []
        mediana = []
        for l in np.array(lstDATONIB):
            minimo.append(dat[:, l].min())
            massimo.append(dat[:, l].max())
            media.append(dat[:, l].mean())
            mediana.append(np.median(dat[:, l], axis=0))
        plt.plot(minimo[1:], label='minimum')
        plt.plot(massimo[1:], label='maximum')
        plt.plot(media[1:], label='mean')
        plt.plot(mediana[1:], 'r--', label='median')
        plt.legend()
        plt.title('step 2: a (random) threshold is assigned')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()

        dat = np.array(DATONIC)
        for n in dat:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 3: introducing differences between levels')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datcumulativeC = np.array(cumulativeC)
        for n in datcumulativeC:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 3: introducing differences between levels')
        plt.xlabel('Time steps')
        plt.ylabel('Cumulative adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datopinionC = np.array(opinionC)
        for n in datopinionC:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 3: introducing differences between levels')
        plt.xlabel('Time steps')
        plt.ylabel('Average Opinion')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        lstDATONIC = []
        n = len(tutto()[1][0])
        for i in range(n + 0):
            lstDATONIC.append(i)
        minimo = []
        massimo = []
        media = []
        mediana = []
        for l in np.array(lstDATONIC):
            minimo.append(dat[:, l].min())
            massimo.append(dat[:, l].max())
            media.append(dat[:, l].mean())
            mediana.append(np.median(dat[:, l], axis=0))
        plt.plot(minimo[1:], label='minimum')
        plt.plot(massimo[1:], label='maximum')
        plt.plot(media[1:], label='mean')
        plt.plot(mediana[1:], 'r--', label='median')
        plt.legend()
        plt.title('step 3: introducing differences between levels')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()

        dat = np.array(DATONID)
        for n in dat:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 4: introducing differences between Simmelian and non simmelian (sigma)')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datcumulativeD = np.array(cumulativeD)
        for n in datcumulativeD:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 4: introducing differences between Simmelian and non simmelian (sigma)')
        plt.xlabel('Time steps')
        plt.ylabel('Cumulative adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datopinionD = np.array(opinionD)
        for n in datopinionD:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 4: introducing differences between Simmelian and non simmelian (sigma)')
        plt.xlabel('Time steps')
        plt.ylabel('Average Opinion')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        lstDATONID = []
        n = len(tutto()[1][0])
        for i in range(n + 0):
            lstDATONID.append(i)
        minimo = []
        massimo = []
        media = []
        mediana = []
        for l in np.array(lstDATONID):
            minimo.append(dat[:, l].min())
            massimo.append(dat[:, l].max())
            media.append(dat[:, l].mean())
            mediana.append(np.median(dat[:, l], axis=0))
        plt.plot(minimo[1:], label='minimum')
        plt.plot(massimo[1:], label='maximum')
        plt.plot(media[1:], label='mean')
        plt.plot(mediana[1:], 'r--', label='median')
        plt.legend()
        plt.title('step 4: introducing differences between Simmelian and non simmelian (sigma)')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()

        dat = np.array(DATONIE)
        for n in dat:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 5: introducing differences between Simmelian and non simmelian (sigma and mean)')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datcumulativeE = np.array(cumulativeE)
        for n in datcumulativeE:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 5: introducing differences between Simmelian and non simmelian (sigma and mean)')
        plt.xlabel('Time steps')
        plt.ylabel('Cumulative adoption rate')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.legend()
        plt.grid()
        plt.show()
        datopinionE = np.array(opinionE)
        for n in datopinionE:
            if n[0] < 1:
                plt.plot(n[1:], 'k', alpha=0.1)
            if n[0] > 1:
                plt.plot(n[1:], 'r', alpha=0.1)
        plt.title('step 5: introducing differences between Simmelian and non simmelian (sigma and mean')
        plt.xlabel('Time steps')
        plt.ylabel('Average Opinion')
        plt.axhline(y=thr, color='purple', label='threshold (avg)')
        plt.show()
        plt.grid()
        plt.cla()
        lstDATONIE = []
        n = len(tutto()[1][0])
        for i in range(n + 0):
            lstDATONIE.append(i)
        minimo = []
        massimo = []
        media = []
        mediana = []
        for l in np.array(lstDATONIE):
            minimo.append(dat[:, l].min())
            massimo.append(dat[:, l].max())
            media.append(dat[:, l].mean())
            mediana.append(np.median(dat[:, l], axis=0))
        plt.plot(minimo[1:], label='minimum')
        plt.plot(massimo[1:], label='maximum')
        plt.plot(media[1:], label='mean')
        plt.plot(mediana[1:], 'r--', label='median')
        plt.legend()
        plt.title('step 5: introducing differences between Simmelian and non simmelian (sigma and mean)')
        plt.xlabel('Time steps')
        plt.ylabel('Actual adoption rate')
        plt.grid()
        plt.show()




#contaggion(50,4,1000,10,1)
#parameter_analysis(50, 4, 1000,100,0.2,1)
#parameter_analys_fix(50, 4, 1000,100,0.2,1)
#opinion_dynamics(50,4,1000,0.1,100,0.2,0,1)
#seeding(50, 4, 1000,100,0.2,1)

